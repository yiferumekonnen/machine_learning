{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392278d5-e024-47e8-bbca-fc57447009bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names and data types:\n",
      "Date_GC               object\n",
      "Date_EC               object\n",
      "U1_pr                 object\n",
      "U2_pr                 object\n",
      "Max_ALoad            float64\n",
      "Min_ALoad            float64\n",
      "Auxiliary            float64\n",
      "Water_Level          float64\n",
      "T2M                  float64\n",
      "PRECTOTCORR          float64\n",
      "Daily discharge       object\n",
      "Actuall energy       float64\n",
      "ALLSKY_SFC_SW_DWN    float64\n",
      "RH2M                 float64\n",
      "WS2M                 float64\n",
      "dtype: object\n",
      "\n",
      "Columns in DataFrame after cleaning and adding lagged columns: ['Date_GC', 'Date_EC', 'U1_pr', 'U2_pr', 'Max_ALoad', 'Min_ALoad', 'Auxiliary', 'Water_Level', 'T2M', 'PRECTOTCORR', 'Daily discharge', 'Actuall energy', 'ALLSKY_SFC_SW_DWN', 'RH2M', 'WS2M', 'PRECTOTCORR_lag110', 'T2M_lag60', 'ALLSKY_SFC_SW_DWN_lag90', 'water_level_last_year']\n",
      "\n",
      "VIF Results:\n",
      "                    Feature           VIF\n",
      "0               Water_Level  7.317582e+05\n",
      "1                       T2M  1.540211e+02\n",
      "2               PRECTOTCORR  2.284075e+00\n",
      "3         ALLSKY_SFC_SW_DWN  7.081200e+01\n",
      "4                      RH2M  4.209692e+01\n",
      "5                      WS2M  1.570806e+01\n",
      "6        PRECTOTCORR_lag110  1.869798e+00\n",
      "7                 T2M_lag60  1.038061e+02\n",
      "8   ALLSKY_SFC_SW_DWN_lag90  3.692120e+01\n",
      "9     water_level_last_year  7.355361e+05\n",
      "10                    U1_pr           inf\n",
      "11                    U2_pr           inf\n",
      "12                Max_ALoad  2.861776e+01\n",
      "13                Min_ALoad  1.110370e+01\n",
      "14                Auxiliary  1.041995e+01\n",
      "15          Daily discharge  1.707604e+00\n",
      "16           Actuall energy           inf\n",
      "\n",
      "VIF Interpretation:\n",
      "VIF < 5: Low multicollinearity\n",
      "5 ‚â§ VIF < 10: Moderate multicollinearity\n",
      "VIF ‚â• 10: High multicollinearity\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Load your dataset (replace 'your_data.csv' with the actual file path)\n",
    "# Example for CSV; adjust if your data is in another format (e.g., Excel, SQL, etc.)\n",
    "df = pd.read_csv(r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Amerti Neshi.csv\")  # Update with your file path\n",
    "# Print column names and their data types to diagnose issues\n",
    "print(\"Column names and data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Ensure 'Date_GC' is parsed as datetime\n",
    "df['Date_GC'] = pd.to_datetime(df['Date_GC'], errors='coerce')\n",
    "\n",
    "# Columns to clean for numeric conversion\n",
    "numeric_cols = ['U1_pr', 'U2_pr', 'Max_ALoad', 'Min_ALoad', 'Auxiliary', 'Daily discharge', 'Actuall energy']\n",
    "\n",
    "# Clean numeric columns: apply .str.replace only to string columns\n",
    "for col in numeric_cols:\n",
    "    if df[col].dtype == 'object':  # Check if column is string (object)\n",
    "        df[col] = pd.to_numeric(df[col].str.replace(',', ''), errors='coerce')\n",
    "    else:\n",
    "        # If already numeric, ensure it's float\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Create lagged columns\n",
    "df['PRECTOTCORR_lag110'] = df['PRECTOTCORR'].shift(110)  # Lag by 110 days\n",
    "df['T2M_lag60'] = df['T2M'].shift(60)  # Lag by 60 days\n",
    "df['ALLSKY_SFC_SW_DWN_lag90'] = df['ALLSKY_SFC_SW_DWN'].shift(90)  # Lag by 90 days\n",
    "df['water_level_last_year'] = df['Water_Level'].shift(365)  # Lag by 365 days (1 year)\n",
    "\n",
    "# Print column names to verify\n",
    "print(\"\\nColumns in DataFrame after cleaning and adding lagged columns:\", df.columns.tolist())\n",
    "\n",
    "# Define numerical columns for VIF calculation\n",
    "numerical_columns = [\n",
    "    'Water_Level', 'T2M', 'PRECTOTCORR', 'ALLSKY_SFC_SW_DWN', \n",
    "    'RH2M', 'WS2M', 'PRECTOTCORR_lag110', 'T2M_lag60', \n",
    "    'ALLSKY_SFC_SW_DWN_lag90', 'water_level_last_year',\n",
    "    'U1_pr', 'U2_pr', 'Max_ALoad', 'Min_ALoad', 'Auxiliary', \n",
    "    'Daily discharge', 'Actuall energy'\n",
    "]\n",
    "\n",
    "# Check if all specified columns exist and are numeric\n",
    "missing_columns = [col for col in numerical_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Error: The following columns are missing in the DataFrame: {missing_columns}\")\n",
    "    print(\"Please check the column names or update the 'numerical_columns' list.\")\n",
    "else:\n",
    "    # Verify that all columns are numeric\n",
    "    non_numeric_cols = [col for col in numerical_columns if not pd.api.types.is_numeric_dtype(df[col])]\n",
    "    if non_numeric_cols:\n",
    "        print(f\"Error: The following columns are not numeric: {non_numeric_cols}\")\n",
    "    else:\n",
    "        # Create a DataFrame with only numerical columns and drop rows with any missing values\n",
    "        df_vif = df[numerical_columns].dropna()\n",
    "\n",
    "        # Calculate VIF for each feature\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data['Feature'] = numerical_columns\n",
    "        vif_data['VIF'] = [variance_inflation_factor(df_vif.values, i) for i in range(df_vif.shape[1])]\n",
    "\n",
    "        # Display VIF results\n",
    "        print(\"\\nVIF Results:\")\n",
    "        print(vif_data)\n",
    "\n",
    "        # Interpretation of VIF values\n",
    "        print(\"\\nVIF Interpretation:\")\n",
    "        print(\"VIF < 5: Low multicollinearity\")\n",
    "        print(\"5 ‚â§ VIF < 10: Moderate multicollinearity\")\n",
    "        print(\"VIF ‚â• 10: High multicollinearity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf57fd7b-455a-49b2-beec-94c6b13c7403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation with Water_Level:\n",
      "           Variable  Correlation_with_Water_Level\n",
      "          Min_ALoad                      0.454066\n",
      " PRECTOTCORR_lag110                      0.302771\n",
      "     Actuall energy                      0.277016\n",
      "          Max_ALoad                      0.141454\n",
      "  ALLSKY_SFC_SW_DWN                      0.045445\n",
      "               RH2M                      0.012227\n",
      "                T2M                     -0.056987\n",
      "               WS2M                     -0.062228\n",
      "          Auxiliary                     -0.066924\n",
      "        PRECTOTCORR                     -0.135443\n",
      "          T2M_lag60                     -0.230205\n",
      "ALLSKY_SFC_SW_DWN60                     -0.334184\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Data Source Configurations ---\n",
    "POWER_PLANT_DATA = {\n",
    "    \n",
    "    'Amerti': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Amerti Neshi.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max_load', 'min_load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', \n",
    "                         'water level_last_year', 'total-pr_last_year', \n",
    "                         'max load_last_year', 'min load_last_year', \n",
    "                         'monthly_evap', 'seasonal_humidity']\n",
    "    }\n",
    "    # Add more plants if needed\n",
    "}\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "def preprocess_data(df, date_col):\n",
    "    if df.empty or date_col not in df.columns:\n",
    "        return df\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    df = df.dropna(subset=[date_col]).sort_values(date_col)\n",
    "    for col in df.columns:\n",
    "        if col != date_col:\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')\n",
    "    if len(df) < 2:\n",
    "        return pd.DataFrame()\n",
    "    return df\n",
    "\n",
    "def load_default_data(power_plant):\n",
    "    if power_plant not in POWER_PLANT_DATA:\n",
    "        return pd.DataFrame(), None, f\"Power plant {power_plant} not found.\"\n",
    "    config = POWER_PLANT_DATA[power_plant]\n",
    "    file_path, date_col = config['path'], config['date_col']\n",
    "    if not os.path.exists(file_path):\n",
    "        return pd.DataFrame(), None, f\"File not found for {power_plant} at {file_path}.\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_filtered = df[df['Power_Plant_Name'] == power_plant].copy() if 'Power_Plant_Name' in df.columns else df.copy()\n",
    "        df_filtered = preprocess_data(df_filtered, date_col)\n",
    "        if df_filtered.empty:\n",
    "            return pd.DataFrame(), None, f\"No valid data after preprocessing for {power_plant}.\"\n",
    "        return df_filtered, date_col, None\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame(), None, f\"Error loading data for {power_plant}: {e}\"\n",
    "\n",
    "# --- Function to Analyze Numeric Relationships Only ---\n",
    "def analyze_variable_relationships(power_plant='Gibe 1'):\n",
    "    # Load data\n",
    "    df, date_col, error = load_default_data(power_plant)\n",
    "    if df.empty:\n",
    "        return f\"Error: {error}\"\n",
    "    \n",
    "    # Identify Water_Level column (case-insensitive)\n",
    "    water_level_col = next((col for col in df.columns \n",
    "                            if 'water_level' in col.lower() and pd.api.types.is_numeric_dtype(df[col])), None)\n",
    "    if not water_level_col:\n",
    "        return f\"No water level column found in {power_plant} dataset.\"\n",
    "    \n",
    "    # Add lag features if enough rows\n",
    "    if len(df) > 110:\n",
    "        t2m_cols = [col for col in df.columns if 't2m' in col.lower() and pd.api.types.is_numeric_dtype(df[col])]\n",
    "        if t2m_cols:\n",
    "            df['T2M_lag60'] = df[t2m_cols[0]].shift(60)\n",
    "        precip_cols = [col for col in df.columns if 'prectotcorr' in col.lower() and pd.api.types.is_numeric_dtype(df[col])]\n",
    "        if precip_cols:\n",
    "            df['PRECTOTCORR_lag110'] = df[precip_cols[0]].shift(110)\n",
    "        sw_cols = [col for col in df.columns if 'allsky_sfc_sw_dwn' in col.lower() and pd.api.types.is_numeric_dtype(df[col])]\n",
    "        if sw_cols:\n",
    "            df['ALLSKY_SFC_SW_DWN60'] = df[sw_cols[0]].shift(90)\n",
    "    \n",
    "    # Get numeric columns (excluding date + water level itself)\n",
    "    numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in [date_col, water_level_col]]\n",
    "    if not numeric_cols:\n",
    "        return f\"No numeric columns available to analyze relationships with {water_level_col}.\"\n",
    "    \n",
    "    # Compute correlations\n",
    "    correlations = df[numeric_cols + [water_level_col]].corr()[water_level_col].drop(water_level_col)\n",
    "    corr_df = pd.DataFrame({\n",
    "        'Variable': correlations.index,\n",
    "        'Correlation_with_Water_Level': correlations.values\n",
    "    }).sort_values(by='Correlation_with_Water_Level', ascending=False)\n",
    "    \n",
    "    return corr_df\n",
    "\n",
    "# --- Run Example ---\n",
    "if __name__ == '__main__':\n",
    "    results = analyze_variable_relationships('Amerti')\n",
    "    if isinstance(results, str):\n",
    "        print(results)\n",
    "    else:\n",
    "        print(\"\\nCorrelation with Water_Level:\")\n",
    "        print(results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3125a568-1e27-433b-93b3-3bdef36dea3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:5021/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2217ad6cec0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output, State, dash_table\n",
    "import dash_bootstrap_components as dbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Table, TableStyle, Spacer\n",
    "from reportlab.lib import colors\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- AI Model Placeholders ---\n",
    "def get_gemini_response(prompt, context_data=None):\n",
    "    return \"AI response functionality is currently a placeholder.\"\n",
    "\n",
    "def get_chatgpt_response(prompt, context_data=None):\n",
    "    return \"AI response functionality is currently a placeholder.\"\n",
    "\n",
    "AI_MODELS = {\n",
    "    'google_gemini': {'name': 'Google Gemini', 'function': get_gemini_response},\n",
    "    'chat_gpt': {'name': 'ChatGPT (OpenAI)', 'function': get_chatgpt_response}\n",
    "}\n",
    "\n",
    "def query_ai_model(prompt, model_id='google_gemini', context_data=None):\n",
    "    return AI_MODELS.get(model_id, {}).get('function', lambda p, c: \"Invalid AI model selected.\")(prompt, context_data)\n",
    "\n",
    "# --- Data Source Configurations ---\n",
    "POWER_PLANT_DATA = {\n",
    "    'Gibe 1': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Gibe1.csv\",\n",
    "        'target_vars': ['Water_Level', 'Total_pr', 'Max_ALoad', 'Min_ALoad'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'Water_Level_last_year', 'Total_pr_last_year', 'Max_ALoad_last_year', 'Min_ALoad_last_year', 'monthly_precip_avg', 'seasonal_temp_avg']\n",
    "    },\n",
    "    'Amerti': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Amerti Neshi.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max_load', 'min_load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'water level_last_year', 'total-pr_last_year', 'max load_last_year', 'min load_last_year', 'monthly_evap', 'seasonal_humidity']\n",
    "    },\n",
    "    'GERD': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\GERD.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max_load', 'min_load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'water_level_last_year', 'total-pr_last_year', 'max_load_last_year', 'min_load_last_year', 'solar_radiation', 'cloud_cover_index']\n",
    "    },\n",
    "    'Gibe3': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Gibe3.csv\",\n",
    "        'target_vars': ['Water_Level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'water level_last_year', 'total-pr_last_year', 'max load_last_year', 'min load_last_year', 'dew_point', 'evapotranspiration']\n",
    "    },\n",
    "    'Finchaa': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\fincha.csv\",\n",
    "        'target_vars': ['Water_Level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'water level_last_year', 'total-pr_last_year', 'max load_last_year', 'min load_last_year', 'temp_range', 'rain_days_count']\n",
    "    },\n",
    "    'Gibe 2': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Gibe2.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max_load', 'min_load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'water level_last_year', 'total-pr_last_year', 'max load_last_year', 'min load_last_year', 'wind_direction', 'soil_moisture']\n",
    "    },\n",
    "    'Koka': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Koka Plant.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max_load', 'min_load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'water level_last_year', 'total-pr_last_year', 'max load_last_year', 'min load_last_year', 'humidity_index', 'pressure_trend']\n",
    "    },\n",
    "    'Tana Beles': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Tana_Beles.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'water level_last_year', 'total-pr_last_year', 'max load_last_year', 'min load_last_year', 'reservoir_inflow', 'upstream_level']\n",
    "    },\n",
    "    'Tekeze': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Tekeze.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'water level_last_year', 'total-pr_last_year', 'max load_last_year', 'min_load_last_year', 'snow_melt_rate', 'glacier_index']\n",
    "    },\n",
    "    'Melka Wakena': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\wakena.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max_load', 'min_load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'water level_last_year', 'total-pr_last_year', 'max load_last_year', 'min load_last_year', 'sediment_level', 'turbidity_index']\n",
    "    },\n",
    "    'Awash2': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Awash2.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'water level_last_year', 'total-pr_last_year', 'max load_last_year', 'min load_last_year', 'sediment_level', 'turbidity_index']\n",
    "    },\n",
    "    'Awash3': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Awash3.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS10M', 'water level_last_year', 'total-pr_last_year', 'max load_last_year', 'min load_last_year', 'sediment_level', 'turbidity_index']\n",
    "    },\n",
    "    'Genale': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Genale .csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC',\n",
    "        'feature_cols': ['T2M', 'PRECTOTCORR', 'RH2M', 'PS', 'WS2M', 'water level_last_year', 'total-pr_last_year', 'max load_last_year', 'min load_last_year', 'sediment_level', 'turbidity_index']\n",
    "    }\n",
    "}\n",
    "\n",
    "DATE_COL_PREFIXES = ['Date', 'Record_Date', 'Time', 'Date GC', 'Date EC']\n",
    "FEATURE_COL_PREFIXES = ['T2M', 'PRECTOT', 'Temp', 'Rain', 'Wind', 'Humidity', 'Pressure']\n",
    "EXCLUDE_COLS = ['ID', 'Index', 'Key', 'Power_Plant_Name']\n",
    "ALLOWED_TARGET_KEYWORDS = ['level', 'pr', 'energy', 'discharge', 'auxiliary', 'load', 'u1', 'u2', 'u3', 'u4', 'u5', 'u6']\n",
    "\n",
    "data_cache = {\n",
    "    'default': {'source': 'default', 'preprocessed_dfs': {}, 'power_plants': list(POWER_PLANT_DATA.keys()), 'date_col': None, 'target_col': None, 'task_type': None},\n",
    "    'uploaded': {'source': None, 'type': None, 'df': None, 'preprocessed_dfs': {}, 'power_plants': [], 'date_col': None, 'target_col': None, 'task_type': None, 'pending_file': None, 'pending_filename': None}\n",
    "}\n",
    "\n",
    "# --- Data Processing Functions ---\n",
    "def preprocess_data(df, date_col):\n",
    "    if df.empty or date_col not in df.columns:\n",
    "        return df\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    df = df.dropna(subset=[date_col]).sort_values(date_col)\n",
    "    \n",
    "    # Handle duplicate dates by aggregating numeric columns with mean\n",
    "    if df[date_col].duplicated().any():\n",
    "        df = df.groupby(date_col).mean(numeric_only=True).reset_index()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col != date_col:\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')\n",
    "    \n",
    "    if len(df) < 2:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Reset index to ensure uniqueness\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def load_default_data(power_plant):\n",
    "    if power_plant not in POWER_PLANT_DATA:\n",
    "        return pd.DataFrame(), None, f\"Power plant {power_plant} not found.\"\n",
    "    config = POWER_PLANT_DATA[power_plant]\n",
    "    file_path, date_col = config['path'], config['date_col']\n",
    "    if not os.path.exists(file_path):\n",
    "        return pd.DataFrame(), None, f\"File not found for {power_plant} at {file_path}.\"\n",
    "    try:\n",
    "        if power_plant in data_cache['default']['preprocessed_dfs']:\n",
    "            return data_cache['default']['preprocessed_dfs'][power_plant], date_col, None\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_filtered = df[df['Power_Plant_Name'] == power_plant].copy() if 'Power_Plant_Name' in df.columns else df.copy()\n",
    "        df_filtered = preprocess_data(df_filtered, date_col)\n",
    "        if df_filtered.empty:\n",
    "            return pd.DataFrame(), None, f\"No valid data after preprocessing for {power_plant}.\"\n",
    "        data_cache['default']['preprocessed_dfs'][power_plant] = df_filtered\n",
    "        data_cache['default']['date_col'] = date_col\n",
    "        return df_filtered, date_col, None\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame(), None, f\"Error loading data for {power_plant}: {e}\"\n",
    "\n",
    "def parse_uploaded_file(contents, filename, power_plant):\n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string)\n",
    "    try:\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(io.StringIO(decoded.decode('utf-8')))\n",
    "        elif filename.endswith('.parquet'):\n",
    "            df = pd.read_parquet(io.BytesIO(decoded))\n",
    "        elif filename.endswith(('.xlsx', '.xls')):\n",
    "            df = pd.read_excel(io.BytesIO(decoded))\n",
    "        elif filename.endswith('.json'):\n",
    "            df = pd.read_json(io.StringIO(decoded.decode('utf-8')))\n",
    "        else:\n",
    "            return None, None, [], None, f\"Unsupported file format: {filename}.\"\n",
    "        power_plants = df['Power_Plant_Name'].unique().tolist() if 'Power_Plant_Name' in df.columns else [power_plant or 'Uploaded Data']\n",
    "        date_col = next((col for col in df.columns if any(p.lower() in col.lower() for p in DATE_COL_PREFIXES)), None)\n",
    "        if not date_col:\n",
    "            return None, None, [], None, \"No valid date column found.\"\n",
    "        df = preprocess_data(df, date_col)\n",
    "        if df.empty:\n",
    "            return None, None, [], None, \"No valid data after preprocessing.\"\n",
    "        for plant in power_plants:\n",
    "            data_cache['uploaded']['preprocessed_dfs'][plant] = df[df['Power_Plant_Name'] == plant].copy() if 'Power_Plant_Name' in df.columns else df.copy()\n",
    "        return filename.split('.')[-1], df, power_plants, date_col, None\n",
    "    except Exception as e:\n",
    "        return None, None, [], None, f\"Error processing {filename}: {e}\"\n",
    "\n",
    "def load_plant_data(power_plant, source='default'):\n",
    "    cache = data_cache[source]\n",
    "    if power_plant in cache['preprocessed_dfs']:\n",
    "        return cache['preprocessed_dfs'][power_plant], cache['date_col'], None\n",
    "    if source == 'default':\n",
    "        return load_default_data(power_plant)\n",
    "    return pd.DataFrame(), None, f\"No data available for {power_plant}.\"\n",
    "\n",
    "def get_available_columns(df, date_col, power_plant, source='default'):\n",
    "    if df.empty or date_col is None:\n",
    "        return [], [], None, None\n",
    "    \n",
    "    # Ensure unique index before processing\n",
    "    if df.index.duplicated().any():\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Initialize feature columns\n",
    "    if source == 'default' and power_plant in POWER_PLANT_DATA:\n",
    "        # Start with predefined feature columns, excluding PRECTOTCORR, T2M, and ALLSKY_SFC_SW_DWN\n",
    "        feature_cols = [col for col in POWER_PLANT_DATA[power_plant]['feature_cols'] \n",
    "                        if col in df.columns \n",
    "                        and pd.api.types.is_numeric_dtype(df[col]) \n",
    "                        and col not in ['PRECTOTCORR', 'T2M', 'ALLSKY_SFC_SW_DWN']]\n",
    "    else:\n",
    "        # Identify feature columns based on prefixes, excluding PRECTOTCORR, T2M, and ALLSKY_SFC_SW_DWN\n",
    "        feature_cols = [col for col in df.columns \n",
    "                        if any(p.lower() in col.lower() for p in FEATURE_COL_PREFIXES) \n",
    "                        and pd.api.types.is_numeric_dtype(df[col]) \n",
    "                        and col not in ['PRECTOTCORR', 'T2M', 'ALLSKY_SFC_SW_DWN']]\n",
    "    \n",
    "    # Add lag features if columns exist and data is sufficient\n",
    "    if len(df) > 110:  # Ensure enough data for the longest lag (110 days)\n",
    "        # Add 110-day lag for PRECTOTCORR\n",
    "        precip_cols = [col for col in df.columns if 'prectotcorr' in col.lower() and pd.api.types.is_numeric_dtype(df[col])]\n",
    "        if precip_cols:\n",
    "            df['PRECTOTCORR_lag110'] = df[precip_cols[0]].shift(110)\n",
    "            if pd.api.types.is_numeric_dtype(df['PRECTOTCORR_lag110']):\n",
    "                feature_cols.append('PRECTOTCORR_lag110')\n",
    "        \n",
    "        # Add 60-day lag for T2M\n",
    "        if len(df) > 60:  # Check for T2M lag\n",
    "            t2m_cols = [col for col in df.columns if 't2m' in col.lower() and pd.api.types.is_numeric_dtype(df[col])]\n",
    "            if t2m_cols:\n",
    "                df['T2M_lag60'] = df[t2m_cols[0]].shift(60)\n",
    "                if pd.api.types.is_numeric_dtype(df['T2M_lag60']):\n",
    "                    feature_cols.append('T2M_lag60')\n",
    "        \n",
    "        # Add 90-day lag for ALLSKY_SFC_SW_DWN\n",
    "        if len(df) > 90:  # Check for ALLSKY lag\n",
    "            allsky_cols = [col for col in df.columns if 'allsky_sfc_sw_dwn' in col.lower() and pd.api.types.is_numeric_dtype(df[col])]\n",
    "            if allsky_cols:\n",
    "                df['ALLSKY_SFC_SW_DWN_lag90'] = df[allsky_cols[0]].shift(90)\n",
    "                if pd.api.types.is_numeric_dtype(df['ALLSKY_SFC_SW_DWN_lag90']):\n",
    "                    feature_cols.append('ALLSKY_SFC_SW_DWN_lag90')\n",
    "    \n",
    "    # Add water_level_last_year if water_level or similar column exists\n",
    "    water_level_cols = [col for col in df.columns if 'water_level' in col.lower() and pd.api.types.is_numeric_dtype(df[col])]\n",
    "    if water_level_cols and len(df) > 365:  # Ensure enough data for yearly lag\n",
    "        water_level_col = water_level_cols[0]  # Use the first matching water level column\n",
    "        df['water_level_last_year'] = df[water_level_col].shift(365)\n",
    "        if pd.api.types.is_numeric_dtype(df['water_level_last_year']):\n",
    "            feature_cols.append('water_level_last_year')\n",
    "    \n",
    "    # Identify potential target columns\n",
    "    potential_targets = [col for col in df.columns if col not in [date_col] + feature_cols + EXCLUDE_COLS and 'date ec' not in col.lower() and any(k in col.lower() for k in ALLOWED_TARGET_KEYWORDS)]\n",
    "    numeric_targets = [col for col in potential_targets if pd.api.types.is_numeric_dtype(df[col])]\n",
    "    categorical_targets = [col for col in potential_targets if not pd.api.types.is_numeric_dtype(df[col])]\n",
    "    target_options = numeric_targets + categorical_targets\n",
    "    \n",
    "    # Select default target column\n",
    "    if source == 'default' and power_plant in POWER_PLANT_DATA:\n",
    "        target_cols = [col for col in POWER_PLANT_DATA[power_plant]['target_vars'] if col in target_options]\n",
    "        target_col = target_cols[0] if target_cols else (numeric_targets[0] if numeric_targets else (categorical_targets[0] if categorical_targets else None))\n",
    "    else:\n",
    "        target_col = numeric_targets[0] if numeric_targets else (categorical_targets[0] if categorical_targets else None)\n",
    "    \n",
    "    # Determine task type\n",
    "    task_type = 'regression' if target_col in numeric_targets else ('classification' if target_col in categorical_targets else None)\n",
    "    \n",
    "    return feature_cols, target_options, target_col, task_type\n",
    "\n",
    "def create_features(df_input, date_col):\n",
    "    df = df_input.copy()\n",
    "    # Ensure unique index before feature creation\n",
    "    if df.index.duplicated().any():\n",
    "        df = df.reset_index(drop=True)\n",
    "    df['year'] = df[date_col].dt.year\n",
    "    df['month'] = df[date_col].dt.month\n",
    "    df['day'] = df[date_col].dt.day\n",
    "    df['dayofyear'] = df[date_col].dt.dayofyear\n",
    "    df['weekday'] = df[date_col].dt.weekday\n",
    "    return df\n",
    "\n",
    "def evaluate_regression_models(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf', C=1.0),\n",
    "        'KNN': KNeighborsRegressor(n_neighbors=5),\n",
    "        'XGBoost': XGBRegressor(n_estimators=30, learning_rate=0.1, max_depth=5, n_jobs=-1, random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=30, max_depth=10, n_jobs=-1, random_state=42),\n",
    "        'Linear Regression': LinearRegression(),\n",
    "    }\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred, y_test_pred = model.predict(X_train), model.predict(X_test)\n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'R2_Train': r2_score(y_train, y_train_pred),\n",
    "                'R2_Test': r2_score(y_test, y_test_pred),\n",
    "                'MAE_Train': mean_absolute_error(y_train, y_train_pred),\n",
    "                'MAE_Test': mean_absolute_error(y_test, y_test_pred),\n",
    "                'RMSE_Train': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "                'RMSE_Test': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "                '_model_obj': model,\n",
    "                '_y_test_pred': y_test_pred,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Model {name} failed: {e}\")\n",
    "    if not results:\n",
    "        return (None, None), pd.DataFrame(), []\n",
    "    results_df = pd.DataFrame(results).sort_values(by='R2_Test', ascending=False).reset_index(drop=True)\n",
    "    best_model_row = results_df.iloc[0]\n",
    "    best_model = (best_model_row['Model'], best_model_row['_model_obj'])\n",
    "    y_test_predictions = best_model_row['_y_test_pred']\n",
    "    return best_model, results_df.drop(columns=['_model_obj', '_y_test_pred']), y_test_predictions\n",
    "\n",
    "def evaluate_classification_models(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "        'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=30, max_depth=10, n_jobs=-1, random_state=42),\n",
    "        'SVC': SVC(probability=True, random_state=42),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),\n",
    "        'XGBoost': XGBClassifier(n_estimators=30, learning_rate=0.1, max_depth=5, n_jobs=-1, use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    }\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred, y_test_pred = model.predict(X_train), model.predict(X_test)\n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'Accuracy_Train': accuracy_score(y_train, y_train_pred),\n",
    "                'Accuracy_Test': accuracy_score(y_test, y_test_pred),\n",
    "                'F1_Train': f1_score(y_train, y_train_pred, average='weighted', zero_division=0),\n",
    "                'F1_Test': f1_score(y_test, y_test_pred, average='weighted', zero_division=0),\n",
    "                '_model_obj': model,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Classification model {name} failed: {e}\")\n",
    "    if not results:\n",
    "        return (None, None), pd.DataFrame()\n",
    "    results_df = pd.DataFrame(results).sort_values(by='F1_Test', ascending=False).reset_index(drop=True)\n",
    "    best_model_row = results_df.iloc[0]\n",
    "    best_model = (best_model_row['Model'], best_model_row['_model_obj'])\n",
    "    return best_model, results_df.drop(columns=['_model_obj'])\n",
    "\n",
    "# --- Dash App Setup ---\n",
    "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP, 'https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css'])\n",
    "app.title = \"Ethiopian Power Plant Forecast Dashboard\"\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Store(id='intermediate-data-store'),\n",
    "    dbc.Modal([\n",
    "        dbc.ModalHeader(\"High Deviation Forecast Alerts\"),\n",
    "        dbc.ModalBody(id='notification-body'),\n",
    "        dbc.ModalFooter(dbc.Button(\"Close\", id=\"close-notification-modal\", className=\"ml-auto\")),\n",
    "    ], id=\"notification-modal\", is_open=False, size=\"xl\"),\n",
    "    \n",
    "    html.Div([\n",
    "        html.H2(\"üìä Ethiopian Electric Power Plant Forecast\", style={'textAlign': 'left', 'fontWeight': 'bold', 'display': 'inline-block'}),\n",
    "        html.Div([\n",
    "            html.Label(\"Upload Dataset\", style={'fontWeight': 'bold', 'color': 'red'}),\n",
    "            dcc.Upload(\n",
    "                id='upload-data',\n",
    "                children=html.Button('Upload File', style={'padding': '5px', 'backgroundColor': '#007BFF', 'color': 'white', 'borderRadius': '5px'}),\n",
    "                multiple=False, accept='.csv,.xlsx,.xls,.json,.parquet'\n",
    "            ),\n",
    "            html.Div(id='upload-status', style={'marginTop': '5px', 'color': 'green'}),\n",
    "        ], style={'display': 'inline-block', 'marginLeft': 'auto', 'marginRight': '20px', 'textAlign': 'right'}),\n",
    "        html.Div([\n",
    "            dbc.Button(id='notification-icon', n_clicks=0, color=\"secondary\", className=\"position-relative\", style={'fontSize': '1.2rem', 'padding': '5px 10px'}),\n",
    "        ], style={'display': 'inline-block', 'verticalAlign': 'top', 'paddingTop': '5px'})\n",
    "    ], style={'display': 'flex', 'alignItems': 'center', 'padding': '20px'}),\n",
    "    \n",
    "    html.Div([\n",
    "        html.Div([html.Label(\"Select Power Plant:\", style={'fontWeight': 'bold'}), dcc.Dropdown(id='power-plant-selector', options=[{'label': p, 'value': p} for p in POWER_PLANT_DATA.keys()], value='Gibe 1', clearable=False)], style={'width': '24%', 'display': 'inline-block', 'marginRight': '1%'}),\n",
    "        html.Div([html.Label(\"Select Target Variable:\", style={'fontWeight': 'bold'}), dcc.Dropdown(id='target-selector', placeholder=\"Select a target variable\")], style={'width': '24%', 'display': 'inline-block', 'marginRight': '1%'}),\n",
    "        html.Div([html.Label(\"Train-Test Split:\", style={'fontWeight': 'bold'}), dcc.Dropdown(id='split-ratio-selector', options=[{'label': f'{i}% Train', 'value': i/100} for i in range(60, 91, 10)], value=0.7, clearable=False)], style={'width': '24%', 'display': 'inline-block', 'marginRight': '1%'}),\n",
    "        html.Div([html.Label(\"Forecast Period:\", style={'fontWeight': 'bold'}), dcc.Dropdown(id='horizon-period-selector', options=[{'label': 'Daily', 'value': 'daily'}, {'label': 'Weekly', 'value': 'weekly'}, {'label': 'Monthly', 'value': 'monthly'}, {'label': 'Yearly', 'value': 'yearly'}], value='monthly', clearable=False), html.Div(id='daily-options', children=[dcc.Dropdown(id='day-of-month-selector', placeholder='Select Day(s) of Month', options=[{'label': str(d), 'value': d} for d in range(1, 32)], multi=True)], style={'display': 'none', 'marginTop': '5px'}), html.Div(id='month-year-options', children=[dcc.Dropdown(id='month-selector', placeholder='Select Months', options=[{'label': datetime(2000, i, 1).strftime('%B'), 'value': i} for i in range(1, 13)], multi=True)], style={'display': 'none', 'marginTop': '5px'}), html.Div(id='year-options', children=[dcc.Dropdown(id='year-selector', placeholder='Select Year', options=[{'label': str(y), 'value': y} for y in range(2025, 2031)], value=2025)], style={'marginTop': '5px'})], style={'width': '24%', 'display': 'inline-block'})\n",
    "    ], style={'padding': '20px', 'display': 'flex', 'justifyContent': 'space-between', 'alignItems': 'flex-start'}),\n",
    "    \n",
    "    html.Hr(),\n",
    "    html.H3(\"Action Center\", className=\"text-center my-4\"),\n",
    "    dbc.Row([dbc.Col(dcc.Dropdown(id='action-selector', options=[\n",
    "        {'label': 'üí¨ Ask Any Questions (AI Assistant)', 'value': 'ask_questions'},\n",
    "        {'label': 'üõ†Ô∏è Data Preprocessing', 'value': 'data_preprocessing'},\n",
    "        {'label': 'üîç Feature Selection', 'value': 'feature_selection'},\n",
    "        {'label': 'üìà Data Visualization/Analysis', 'value': 'data_visualization'},\n",
    "        {'label': 'üìÑ Report Generating', 'value': 'report_generating'},\n",
    "        {'label': 'üìÑ Recommendation', 'value': 'recommendation'},\n",
    "    ], placeholder=\"Select an action to perform...\"), width=12)]),\n",
    "    dbc.Card(dbc.CardBody(id='action-output'), className=\"mt-4\"),\n",
    "    \n",
    "    dcc.Graph(id='forecast-graph'),\n",
    "    \n",
    "    html.Div([\n",
    "        html.Button(\"Download Forecast (CSV)\", id='download-btn', style={'padding': '10px 20px', 'backgroundColor': '#28A745', 'color': 'white', 'borderRadius': '5px', 'marginRight': '10px'}),\n",
    "        dcc.Download(id=\"download-forecast\"),\n",
    "        html.Button(\"View Forecast Data\", id='view-btn', style={'padding': '10px 20px', 'backgroundColor': '#007BFF', 'color': 'white', 'borderRadius': '5px', 'marginRight': '10px'}),\n",
    "        html.Button(\"Download Report (PDF)\", id='download-pdf-btn', style={'padding': '10px 20px', 'backgroundColor': '#FF5733', 'color': 'white', 'borderRadius': '5px'}),\n",
    "        dcc.Download(id=\"download-pdf\"),\n",
    "    ], style={'padding': '10px', 'textAlign': 'center'}),\n",
    "    \n",
    "    html.Div(id='view-data-container', children=[\n",
    "        html.Div(id='view-data-display', style={'marginTop': '10px', 'padding': '20px', 'overflowX': 'auto', 'backgroundColor': '#F8F9FA', 'borderRadius': '8px', 'boxShadow': '2px 2px 8px rgba(0,0,0,0.1)'}),\n",
    "        html.Button(\"Close View\", id='close-view-btn', style={'padding': '5px 10px', 'backgroundColor': '#DC3545', 'color': 'white', 'borderRadius': '5px', 'marginTop': '10px'})\n",
    "    ], style={'display': 'none', 'padding': '20px'}),\n",
    "\n",
    "    html.Div(id='model-metrics', style={'padding': '20px'}),\n",
    "    html.Div([\n",
    "        html.Label(\"Raw Dataset Inspector:\", style={'fontWeight': 'bold', 'padding': '5px', 'backgroundColor': '#007BFF', 'color': 'white', 'borderRadius': '5px'}),\n",
    "        dcc.Dropdown(id='data-display-selector', options=[{'label': 'Full Dataset', 'value': 'full'}, {'label': 'First 5 Rows', 'value': 'head'}, {'label': 'Last 5 Rows', 'value': 'tail'}, {'label': 'Info', 'value': 'info'}, {'label': 'Describe', 'value': 'describe'}], value='head', clearable=False)\n",
    "    ], style={'padding': '10px', 'width': '30%', 'margin': '0 auto'}),\n",
    "    html.Div(id='data-display', style={'padding': '20px', 'overflowX': 'auto', 'backgroundColor': '#F8F9FA', 'borderRadius': '8px', 'boxShadow': '2px 2px 8px rgba(0,0,0,0.1)'})\n",
    "])\n",
    "\n",
    "# --- Notification and Badge Callbacks ---\n",
    "@app.callback(\n",
    "    Output('notification-modal', 'is_open'),\n",
    "    [Input('notification-icon', 'n_clicks'), Input('close-notification-modal', 'n_clicks')],\n",
    "    [State('notification-modal', 'is_open')],\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def toggle_notification_modal(n1, n2, is_open):\n",
    "    ctx = dash.callback_context\n",
    "    if not ctx.triggered:\n",
    "        return is_open\n",
    "    button_id = ctx.triggered[0]['prop_id'].split('.')[0]\n",
    "    if button_id in ['notification-icon', 'close-notification-modal']:\n",
    "        return not is_open\n",
    "    return is_open\n",
    "\n",
    "@app.callback(\n",
    "    Output('notification-body', 'children'),\n",
    "    Input('notification-modal', 'is_open'),\n",
    "    State('intermediate-data-store', 'data')\n",
    ")\n",
    "def update_notification_body(is_open, stored_data):\n",
    "    if not is_open or not stored_data or 'anomaly_details' not in stored_data or not stored_data['anomaly_details']:\n",
    "        return html.P(\"No high-deviation alerts to display.\")\n",
    "    notifications = stored_data['anomaly_details']\n",
    "    df = pd.DataFrame(notifications)\n",
    "    df.rename(columns={'gc_date_actual': 'GC Date of Actual', 'actual_data': 'Actual Data', 'forecasted_data': 'Forecasted Data', 'error': 'Error', 'gc_date_forecast': 'GC Date of Forecasted Data'}, inplace=True)\n",
    "    return dash_table.DataTable(data=df.to_dict('records'), columns=[{'name': i, 'id': i} for i in df.columns], page_size=10, style_cell={'textAlign': 'left'}, style_header={'fontWeight': 'bold', 'backgroundColor': '#f8f9fa'}, style_data_conditional=[{'if': {'row_index': 'odd'}, 'backgroundColor': 'rgb(240, 240, 240)'}])\n",
    "\n",
    "@app.callback(\n",
    "    Output('notification-icon', 'children'),\n",
    "    Input('intermediate-data-store', 'data')\n",
    ")\n",
    "def update_notification_badge(stored_data):\n",
    "    base_icon = html.I(className=\"fas fa-bell\")\n",
    "    if not stored_data or 'anomaly_details' not in stored_data or not stored_data['anomaly_details']:\n",
    "        return base_icon\n",
    "    count = len(stored_data['anomaly_details'])\n",
    "    if count == 0:\n",
    "        return base_icon\n",
    "    return html.Span([base_icon, dbc.Badge(f\"{count}\", color=\"danger\", pill=True, className=\"position-absolute top-0 start-100 translate-middle\")])\n",
    "\n",
    "# --- File Upload Callback ---\n",
    "@app.callback(\n",
    "    Output('power-plant-selector', 'options'),\n",
    "    Output('power-plant-selector', 'value'),\n",
    "    Output('upload-status', 'children'),\n",
    "    Input('upload-data', 'contents'),\n",
    "    State('upload-data', 'filename'),\n",
    "    State('power-plant-selector', 'value')\n",
    ")\n",
    "def handle_file_upload(contents, filename, current_plant):\n",
    "    if not contents:\n",
    "        return [{'label': p, 'value': p} for p in POWER_PLANT_DATA.keys()], current_plant or 'Gibe 1', None\n",
    "    file_type, df, power_plants, date_col, error = parse_uploaded_file(contents, filename, None)\n",
    "    if error:\n",
    "        return [{'label': p, 'value': p} for p in POWER_PLANT_DATA.keys()], current_plant or 'Gibe 1', html.P(error, style={'color': 'red'})\n",
    "    data_cache['uploaded']['source'] = filename\n",
    "    data_cache['uploaded']['type'] = file_type\n",
    "    data_cache['uploaded']['df'] = df\n",
    "    data_cache['uploaded']['power_plants'] = power_plants\n",
    "    data_cache['uploaded']['date_col'] = date_col\n",
    "    options = [{'label': p, 'value': p} for p in POWER_PLANT_DATA.keys()] + [{'label': p, 'value': f'uploaded_{p}'} for p in power_plants]\n",
    "    return options, power_plants[0] if power_plants else current_plant, html.P(f\"Uploaded {filename} successfully!\", style={'color': 'green'})\n",
    "\n",
    "# --- Main Forecasting Callback ---\n",
    "@app.callback(\n",
    "    Output('forecast-graph', 'figure'),\n",
    "    Output('model-metrics', 'children'),\n",
    "    Output('intermediate-data-store', 'data'),\n",
    "    Input('power-plant-selector', 'value'),\n",
    "    Input('target-selector', 'value'),\n",
    "    Input('split-ratio-selector', 'value'),\n",
    "    Input('horizon-period-selector', 'value'),\n",
    "    Input('year-selector', 'value'),\n",
    "    State('month-selector', 'value'),\n",
    "    State('day-of-month-selector', 'value')\n",
    ")\n",
    "def update_forecast(power_plant, target, split, horizon, year, months, days):\n",
    "    if not all([power_plant, target, year]):\n",
    "        return go.Figure(), \"Please select all required options.\", {}\n",
    "\n",
    "    source = 'uploaded' if power_plant.startswith('uploaded_') else 'default'\n",
    "    actual_plant = power_plant.replace('uploaded_', '')\n",
    "    df, date_col, err = load_plant_data(actual_plant, source)\n",
    "    if df.empty:\n",
    "        return go.Figure(), f\"Error: {err}\", {}\n",
    "\n",
    "    try:\n",
    "        # Ensure unique index before processing\n",
    "        if df.index.duplicated().any():\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "        if pd.api.types.is_object_dtype(df[target]):\n",
    "            df[target] = pd.to_numeric(df[target].astype(str).str.replace(',', ''), errors='coerce')\n",
    "        df.dropna(subset=[target], inplace=True)\n",
    "        \n",
    "        features, _, _, task = get_available_columns(df, date_col, actual_plant, source)\n",
    "        if not features:\n",
    "            return go.Figure(), \"No valid feature columns found.\", {}\n",
    "        df_model = create_features(df[[date_col] + features + [target]].dropna(), date_col)\n",
    "        if len(df_model) < 20:\n",
    "            return go.Figure(), \"Insufficient data to train model.\", {}\n",
    "        \n",
    "        ext_features = features + ['year', 'month', 'day', 'dayofyear', 'weekday']\n",
    "        train, test = train_test_split(df_model, train_size=split, shuffle=False)\n",
    "        X_train, X_test = train[ext_features], test[ext_features]\n",
    "        y_train_raw, y_test_raw = train[target], test[target]\n",
    "\n",
    "        fig, metrics_display, model_name, forecast_df, notifications = go.Figure(), html.Div(), \"N/A\", pd.DataFrame(), []\n",
    "        metrics_data_for_pdf = []\n",
    "\n",
    "        if task == 'classification':\n",
    "            le = LabelEncoder()\n",
    "            y_train, y_test = le.fit_transform(y_train_raw), le.transform(y_test_raw)\n",
    "            best, results_df = evaluate_classification_models(X_train, X_test, y_train, y_test)\n",
    "            if not best or best[1] is None or results_df.empty:\n",
    "                return go.Figure(), \"Classification models failed.\", {}\n",
    "            model_name, model = best\n",
    "            best_model_name = results_df.iloc[0]['Model']\n",
    "\n",
    "            metrics_data = []\n",
    "            for i, row in results_df.iterrows():\n",
    "                metrics_data.append({\n",
    "                    \"Model\": row['Model'],\n",
    "                    \"Accuracy_Train\": f\"{row['Accuracy_Train']:.4f}\", \"Accuracy_Test\": f\"{row['Accuracy_Test']:.4f}\",\n",
    "                    \"F1_Train\": f\"{row['F1_Train']:.4f}\", \"F1_Test\": f\"{row['F1_Test']:.4f}\",\n",
    "                    \"Level\": i + 1, \"Best\": '‚òÖ' if row['Model'] == best_model_name else ''\n",
    "                })\n",
    "            metrics_data_for_pdf = metrics_data\n",
    "\n",
    "            metrics_display = html.Div([\n",
    "                html.H4(f\"Model Performance (Best: {best_model_name})\"),\n",
    "                dash_table.DataTable(\n",
    "                    columns=[\n",
    "                        {\"name\": [\"\", \"Model\"], \"id\": \"Model\"},\n",
    "                        {\"name\": [\"Accuracy\", \"Train\"], \"id\": \"Accuracy_Train\"}, {\"name\": [\"Accuracy\", \"Test\"], \"id\": \"Accuracy_Test\"},\n",
    "                        {\"name\": [\"F1 Score\", \"Train\"], \"id\": \"F1_Train\"}, {\"name\": [\"F1 Score\", \"Test\"], \"id\": \"F1_Test\"},\n",
    "                        {\"name\": [\"\", \"Level\"], \"id\": \"Level\"}, {\"name\": [\"\", \"Best\"], \"id\": \"Best\"}\n",
    "                    ],\n",
    "                    data=metrics_data, style_cell={'textAlign': 'center'},\n",
    "                    style_header={'fontWeight': 'bold'}, merge_duplicate_headers=True,\n",
    "                    style_data_conditional=[{'if': {'filter_query': '{Best} = \"‚òÖ\"'}, 'backgroundColor': 'rgba(0, 255, 0, 0.2)'}]\n",
    "                )\n",
    "            ])\n",
    "        else: # Regression\n",
    "            best, results_df, y_test_pred = evaluate_regression_models(X_train, X_test, y_train_raw, y_test_raw)\n",
    "            if not best or best[1] is None or results_df.empty:\n",
    "                return go.Figure(), \"Regression models failed.\", {}\n",
    "            model_name, model = best\n",
    "            best_model_name = results_df.iloc[0]['Model']\n",
    "\n",
    "            metrics_data = []\n",
    "            for i, row in results_df.iterrows():\n",
    "                metrics_data.append({\n",
    "                    \"Model\": row['Model'],\n",
    "                    \"R2_Train\": f\"{row['R2_Train']:.4f}\", \"R2_Test\": f\"{row['R2_Test']:.4f}\",\n",
    "                    \"MAE_Train\": f\"{row['MAE_Train']:.4f}\", \"MAE_Test\": f\"{row['MAE_Test']:.4f}\",\n",
    "                    \"RMSE_Train\": f\"{row['RMSE_Train']:.4f}\", \"RMSE_Test\": f\"{row['RMSE_Test']:.4f}\",\n",
    "                    \"Level\": i + 1, \"Best\": '‚òÖ' if row['Model'] == best_model_name else ''\n",
    "                })\n",
    "            metrics_data_for_pdf = metrics_data\n",
    "            \n",
    "            metrics_display = html.Div([\n",
    "                html.H4(f\"Model Performance (Best: {best_model_name})\"),\n",
    "                dash_table.DataTable(\n",
    "                    columns=[\n",
    "                        {\"name\": [\"\", \"Model\"], \"id\": \"Model\"},\n",
    "                        {\"name\": [\"R¬≤\", \"Train\"], \"id\": \"R2_Train\"}, {\"name\": [\"R¬≤\", \"Test\"], \"id\": \"R2_Test\"},\n",
    "                        {\"name\": [\"MAE\", \"Train\"], \"id\": \"MAE_Train\"}, {\"name\": [\"MAE\", \"Test\"], \"id\": \"MAE_Test\"},\n",
    "                        {\"name\": [\"RMSE\", \"Train\"], \"id\": \"RMSE_Train\"}, {\"name\": [\"RMSE\", \"Test\"], \"id\": \"RMSE_Test\"},\n",
    "                        {\"name\": [\"\", \"Level\"], \"id\": \"Level\"}, {\"name\": [\"\", \"Best\"], \"id\": \"Best\"}\n",
    "                    ],\n",
    "                    data=metrics_data, style_cell={'textAlign': 'center'},\n",
    "                    style_header={'fontWeight': 'bold'}, merge_duplicate_headers=True,\n",
    "                    style_data_conditional=[{'if': {'filter_query': '{Best} = \"‚òÖ\"'}, 'backgroundColor': 'rgba(0, 255, 0, 0.2)'}]\n",
    "                )\n",
    "            ])\n",
    "\n",
    "        future_dates = pd.to_datetime([])\n",
    "        if horizon == 'yearly':\n",
    "            future_dates = pd.date_range(start=f\"{year}-01-01\", end=f\"{year}-12-31\", freq='D')\n",
    "        elif months and year:\n",
    "            all_dates = pd.date_range(start=f\"{year}-01-01\", end=f\"{year}-12-31\", freq='D')\n",
    "            future_dates = all_dates[all_dates.month.isin(months)]\n",
    "            if horizon == 'daily' and days:\n",
    "                future_dates = future_dates[future_dates.day.isin(days)]\n",
    "        \n",
    "        if not future_dates.empty:\n",
    "            future_df_features = create_features(pd.DataFrame({date_col: future_dates}), date_col)\n",
    "            for col in features:\n",
    "                future_df_features[col] = df_model[col].median()\n",
    "            future_pred = model.predict(future_df_features[ext_features])\n",
    "            if task == 'classification':\n",
    "                future_pred = le.inverse_transform(future_pred)\n",
    "            forecast_df = pd.DataFrame({'Date': future_dates, f'Forecast_{target}': future_pred})\n",
    "            if task == 'regression':\n",
    "                actual_val = df_model[target].median()\n",
    "                for idx, row in forecast_df.iterrows():\n",
    "                    forecast_val = row[f'Forecast_{target}']\n",
    "                    error = abs(actual_val - forecast_val)\n",
    "                    if error > 5:\n",
    "                        notifications.append({\n",
    "                            'gc_date_actual': \"Historical Median\",\n",
    "                            'actual_data': f\"{actual_val:.2f}\",\n",
    "                            'forecasted_data': f\"{forecast_val:.2f}\",\n",
    "                            'error': f\"{error:.2f}\",\n",
    "                            'gc_date_forecast': row['Date'].strftime('%Y-%m-%d')\n",
    "                        })\n",
    "\n",
    "        if task == 'regression':\n",
    "            fig.add_trace(go.Scatter(x=train[date_col], y=y_train_raw, name='Training Data', mode='lines', line={'color': 'blue'}))\n",
    "            fig.add_trace(go.Scatter(x=test[date_col], y=y_test_raw, name='Test Data (Actual)', mode='lines', line={'color': 'green'}))\n",
    "            fig.add_trace(go.Scatter(x=test[date_col], y=y_test_pred, name='Test Data (Predicted)', mode='lines', line={'color': 'orange', 'dash': 'dot'}))\n",
    "            if not forecast_df.empty:\n",
    "                fig.add_trace(go.Scatter(x=forecast_df['Date'], y=forecast_df[f'Forecast_{target}'], name='Forecast', mode='lines', line={'color': 'red', 'dash': 'dash'}))\n",
    "        else:\n",
    "            classes = le.classes_\n",
    "            fig.add_trace(go.Bar(x=classes, y=y_train_raw.value_counts().reindex(classes, fill_value=0), name='Training Data'))\n",
    "            fig.add_trace(go.Bar(x=classes, y=y_test_raw.value_counts().reindex(classes, fill_value=0), name='Test Data'))\n",
    "            if not forecast_df.empty:\n",
    "                fig.add_trace(go.Bar(x=classes, y=pd.Series(future_pred).value_counts().reindex(classes, fill_value=0), name='Forecast'))\n",
    "\n",
    "        fig.update_layout(title=f'<b>{actual_plant} - {target} Forecast</b>', template='plotly_white')\n",
    "        stored_data = {\n",
    "            'forecast': forecast_df.to_json(orient='split', date_format='iso'),\n",
    "            'metrics': json.dumps(metrics_data_for_pdf),\n",
    "            'historical': test[[date_col, target]].to_json(orient='split', date_format='iso'),\n",
    "            'best_model': best_model_name,\n",
    "            'task_type': task,\n",
    "            'anomaly_details': notifications\n",
    "        }\n",
    "        return fig, metrics_display, stored_data\n",
    "\n",
    "    except Exception as e:\n",
    "        return go.Figure(), f\"An error occurred: {e}\", {}\n",
    "\n",
    "# --- Action Center Callback ---\n",
    "@app.callback(\n",
    "    Output('action-output', 'children'),\n",
    "    Input('action-selector', 'value'),\n",
    "    State('power-plant-selector', 'value'),\n",
    "    State('target-selector', 'value')  # Ensure target variable is captured\n",
    ")\n",
    "def render_action_content(action, power_plant, target_var):\n",
    "    if not action:\n",
    "        return html.P(\"Please select an action from the dropdown above.\", className=\"text-center text-muted\")\n",
    "    \n",
    "    if action == 'ask_questions':\n",
    "        return html.Div([\n",
    "            html.H5(\"üí¨ AI Assistant\", className=\"mb-3\"),\n",
    "            dbc.InputGroup([\n",
    "                dbc.Input(id='question-input', placeholder='Ask about the data, models, or forecasts...'),\n",
    "                dbc.Button(\"Send ‚û§\", id='send-question-btn', color='primary')\n",
    "            ]),\n",
    "            dcc.Loading(html.Div(id='ai-response-area', className=\"mt-3 p-3 border rounded bg-light\"))\n",
    "        ])\n",
    "    \n",
    "    if action == 'data_preprocessing':\n",
    "        if not power_plant:\n",
    "            return html.P(\"Please select a power plant first.\", className=\"text-muted\")\n",
    "        \n",
    "        source = 'uploaded' if power_plant.startswith('uploaded_') else 'default'\n",
    "        actual_plant = power_plant.replace('uploaded_', '')\n",
    "        df, date_col, _ = load_plant_data(actual_plant, source)\n",
    "        if df.empty:\n",
    "            return html.P(f\"No data available for {actual_plant}.\", className=\"text-muted\")\n",
    "        \n",
    "        # Get available target variables\n",
    "        _, target_options, _, _ = get_available_columns(df, date_col, actual_plant, source)\n",
    "        if not target_options:\n",
    "            return html.P(\"No valid target variables found for preprocessing.\", className=\"text-muted\")\n",
    "        \n",
    "        return html.Div([\n",
    "            html.H5(f\"üõ†Ô∏è Data Preprocessing for {actual_plant}\", className=\"mb-3\"),\n",
    "            html.P(f\"Preprocess the data for {actual_plant}. Select a target variable and preprocessing options below.\"),\n",
    "            # Add target variable dropdown for preprocessing\n",
    "            dbc.Row([\n",
    "                dbc.Col(dbc.Label(\"Select Target Variable:\"), width=4),\n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                    id='preprocess-target-selector',\n",
    "                    options=[{'label': col, 'value': col} for col in target_options],\n",
    "                    value=target_var if target_var in target_options else target_options[0] if target_options else None,\n",
    "                    placeholder=\"Select a target variable\",\n",
    "                    clearable=False\n",
    "                ), width=8)\n",
    "            ], className=\"mb-3\"),\n",
    "            dbc.Row([\n",
    "                dbc.Col(dbc.Label(\"Handle Missing Values:\"), width=4),\n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                    id='missing-value-strat',\n",
    "                    options=[\n",
    "                        {'label': 'Drop Missing Values', 'value': 'drop'},\n",
    "                        {'label': 'Impute with Mean', 'value': 'mean'},\n",
    "                        {'label': 'Impute with Median', 'value': 'median'}\n",
    "                    ],\n",
    "                    value='median',\n",
    "                    placeholder=\"Select a strategy\"\n",
    "                ), width=8)\n",
    "            ], className=\"mb-2\"),\n",
    "            dbc.Label(\"Select Additional Preprocessing Steps:\"),\n",
    "            dbc.Checklist(\n",
    "                options=[\n",
    "                    {\"label\": \"Remove Duplicate Rows\", \"value\": \"remove_duplicates\"},\n",
    "                    {\"label\": \"Handle Outliers (IQR Method)\", \"value\": \"handle_outliers\"}\n",
    "                ],\n",
    "                value=[], id=\"preprocess-options\", inline=True, className=\"mb-3\"\n",
    "            ),\n",
    "            dbc.Button(\"Run Preprocessing\", id=\"run-preprocess-btn\", color=\"primary\", className=\"mt-3 w-100\"),\n",
    "            dcc.Loading(html.Div(id=\"preprocess-output-display\", className=\"mt-4\"))\n",
    "        ])\n",
    "    \n",
    "    if action == 'feature_selection':\n",
    "        return html.Div([\n",
    "            html.H5(\"üîç Feature Selection Analysis\", className=\"mb-3\"),\n",
    "            html.P(\"Analyze the features of the currently selected power plant data to identify the most relevant and remove redundant ones.\"),\n",
    "            dbc.Checklist(\n",
    "                options=[{\"label\": \"Target variables\", \"value\": \"target_variables\"}, {\"label\": \"Label variables\", \"value\": \"label_variables\"}],\n",
    "                id=\"feature-options\", inline=True, className=\"mb-3\"\n",
    "            ),\n",
    "            dbc.Button(\"Analyze Features\", id=\"run-feature-select-btn\", color=\"primary\", className=\"w-100\"),\n",
    "            dcc.Loading(html.Div(id=\"feature-select-output\", className=\"mt-4\"))\n",
    "        ])\n",
    "    \n",
    "    if action == 'data_visualization':\n",
    "        if not power_plant:\n",
    "            return html.P(\"Please select a power plant first.\")\n",
    "        source = 'uploaded' if power_plant.startswith('uploaded_') else 'default'\n",
    "        actual_plant = power_plant.replace('uploaded_', '')\n",
    "        df, date_col, _ = load_plant_data(actual_plant, source)\n",
    "        if df.empty:\n",
    "            return html.P(\"No data available for visualization.\")\n",
    "        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        return html.Div([\n",
    "            html.H5(\"üìà Exploratory Data Analysis\", className=\"mb-3\"),\n",
    "            dbc.Row([\n",
    "                dbc.Col(dbc.Label(\"Plot Type:\"), width=2),\n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                    id='plot-type-selector',\n",
    "                    options=[\n",
    "                        {\"label\": \"Histogram\", \"value\": \"Histogram\"},\n",
    "                        {\"label\": \"Scatter Plot\", \"value\": \"Scatter Plot\"},\n",
    "                        {\"label\": \"Correlation Heatmap\", \"value\": \"Correlation Heatmap\"}\n",
    "                    ],\n",
    "                    value='Histogram'\n",
    "                ), width=10)\n",
    "            ]),\n",
    "            html.Div(id='plot-controls', children=[\n",
    "                dcc.Dropdown(id='hist-col', options=[{'label': col, 'value': col} for col in numeric_cols],\n",
    "                             placeholder=\"Select column\", value=numeric_cols[0] if numeric_cols else None, style={'marginTop': '10px'}),\n",
    "                html.Div([\n",
    "                    dcc.Dropdown(id='scatter-x', options=[{'label': col, 'value': col} for col in numeric_cols],\n",
    "                                 placeholder=\"X-axis\", value=numeric_cols[0] if len(numeric_cols) > 0 else None),\n",
    "                    dcc.Dropdown(id='scatter-y', options=[{'label': col, 'value': col} for col in numeric_cols],\n",
    "                                 placeholder=\"Y-axis\", value=numeric_cols[1] if len(numeric_cols) > 1 else None),\n",
    "                ], style={'display': 'none'}, id='scatter-controls')\n",
    "            ]),\n",
    "            dcc.Loading(dcc.Graph(id='eda-plot'))\n",
    "        ])\n",
    "    \n",
    "    if action == 'report_generating':\n",
    "        return html.Div([\n",
    "            html.H5(\"üìÑ Report Generator\", className=\"mb-3\"),\n",
    "            html.P(\"Generate a summary report for the selected power plant including recent data and forecasts.\"),\n",
    "            dbc.Row([\n",
    "                dbc.Col(dbc.Label(\"Report Period:\"), width=3),\n",
    "                dbc.Col(dcc.Dropdown(id='report-period', options=['Monthly', 'Quarterly', 'Yearly'], value='Monthly'), width=9),\n",
    "            ], className=\"mb-2\"),\n",
    "            dbc.Row([\n",
    "                dbc.Col(dbc.Label(\"File format:\"), width=3),\n",
    "                dbc.Col(dcc.Dropdown(id='file-format', options=['pdf'], value='pdf'), width=9),\n",
    "            ], className=\"mb-2\"),\n",
    "            dbc.Button(\"Generate Report\", id=\"generate-report-btn\", color=\"primary\", className=\"w-100 mt-3\"),\n",
    "            html.Div(id='report-status', className=\"mt-3\")\n",
    "        ])\n",
    "    \n",
    "    if action == 'recommendation':\n",
    "        return html.Div([\n",
    "            html.H5(\"üìÑ Recommendations\", className=\"mb-3\"),\n",
    "            html.P(\"Automated recommendations based on forecast trends will appear here.\")\n",
    "        ])\n",
    "    \n",
    "    return html.P(f\"Content for {action} will be loaded here.\")\n",
    "\n",
    "# --- AI and Preprocessing Callbacks ---\n",
    "@app.callback(\n",
    "    Output('ai-response-area', 'children'),\n",
    "    Input('send-question-btn', 'n_clicks'),\n",
    "    [State('question-input', 'value'), State('power-plant-selector', 'value'), State('target-selector', 'value'), State('intermediate-data-store', 'data')]\n",
    ")\n",
    "def answer_question(n_clicks, question, power_plant, target_var, model_results):\n",
    "    if not n_clicks or not question:\n",
    "        return \"Please type a question and click 'Send'.\"\n",
    "    source = 'uploaded' if power_plant.startswith('uploaded_') else 'default'\n",
    "    actual_plant = power_plant.replace('uploaded_', '')\n",
    "    df, _, _ = load_plant_data(actual_plant, source)\n",
    "    context = {\n",
    "        'question': question,\n",
    "        'power_plant': actual_plant,\n",
    "        'target_var': target_var,\n",
    "        'model_results': model_results or {},\n",
    "        'df_head': df.head().to_string() if not df.empty else \"No data.\",\n",
    "        'df_describe': df.describe().to_string() if not df.empty else \"No data.\"\n",
    "    }\n",
    "    ai_answer = query_ai_model(question, 'google_gemini', context)\n",
    "    return dcc.Markdown(ai_answer, dangerously_allow_html=True)\n",
    "\n",
    "@app.callback(\n",
    "    Output('preprocess-output-display', 'children'),\n",
    "    Input('run-preprocess-btn', 'n_clicks'),\n",
    "    [\n",
    "        State('power-plant-selector', 'value'),\n",
    "        State('preprocess-target-selector', 'value'),  # Updated to use preprocess-target-selector\n",
    "        State('missing-value-strat', 'value'),\n",
    "        State('preprocess-options', 'value')\n",
    "    ]\n",
    ")\n",
    "def run_preprocessing(n_clicks, power_plant, target_var, missing_val_strat, options):\n",
    "    if not n_clicks or not power_plant or not target_var:\n",
    "        return html.P(\"Please select a power plant, target variable, and click the button to start.\", className=\"text-muted\")\n",
    "    \n",
    "    try:\n",
    "        source = 'uploaded' if power_plant.startswith('uploaded_') else 'default'\n",
    "        actual_plant = power_plant.replace('uploaded_', '')\n",
    "        df, date_col, _ = load_plant_data(actual_plant, source)\n",
    "        if df.empty:\n",
    "            return html.P(f\"No data available for {actual_plant}.\", className=\"text-muted\")\n",
    "        \n",
    "        processing_log = [f\"Original shape: {df.shape}\"]\n",
    "        rows_before = len(df)\n",
    "        \n",
    "        # Handle missing values\n",
    "        if missing_val_strat == 'drop':\n",
    "            df = df.dropna(subset=[target_var])\n",
    "            processing_log.append(f\"Dropped rows with missing values in {target_var}. New shape: {df.shape}\")\n",
    "        elif missing_val_strat in ['mean', 'median']:\n",
    "            if pd.api.types.is_numeric_dtype(df[target_var]):\n",
    "                impute_value = df[target_var].mean() if missing_val_strat == 'mean' else df[target_var].median()\n",
    "                df[target_var] = df[target_var].fillna(impute_value)\n",
    "                processing_log.append(f\"Imputed missing values in {target_var} with {missing_val_strat}: {impute_value:.2f}\")\n",
    "            else:\n",
    "                return html.P(f\"Cannot impute {target_var} with {missing_val_strat} as it is not numeric.\", className=\"text-danger\")\n",
    "        \n",
    "        # Handle duplicate rows\n",
    "        if 'remove_duplicates' in options:\n",
    "            df = df.drop_duplicates()\n",
    "            processing_log.append(f\"Removed {rows_before - len(df)} duplicate rows. New shape: {df.shape}\")\n",
    "        \n",
    "        # Handle outliers using IQR method\n",
    "        if 'handle_outliers' in options and pd.api.types.is_numeric_dtype(df[target_var]):\n",
    "            Q1 = df[target_var].quantile(0.25)\n",
    "            Q3 = df[target_var].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            rows_before_outliers = len(df)\n",
    "            df = df[(df[target_var] >= lower_bound) & (df[target_var] <= upper_bound)]\n",
    "            processing_log.append(f\"Removed {rows_before_outliers - len(df)} outliers in {target_var}. New shape: {df.shape}\")\n",
    "        \n",
    "        # Update cache\n",
    "        data_cache[source]['preprocessed_dfs'][actual_plant] = df\n",
    "        \n",
    "        return html.Div([\n",
    "            dbc.Alert(f\"Preprocessing Complete for {actual_plant} - {target_var}!\", color=\"success\"),\n",
    "            html.P(f\"Rows before preprocessing: {rows_before}\"),\n",
    "            html.P(f\"Rows after preprocessing: {len(df)}\"),\n",
    "            html.Pre('\\n'.join(processing_log)),\n",
    "            dash_table.DataTable(\n",
    "                data=df.head().to_dict('records'),\n",
    "                columns=[{'name': i, 'id': i} for i in df.columns],\n",
    "                page_size=10,\n",
    "                style_table={'overflowX': 'auto'}\n",
    "            )\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        return dbc.Alert(f\"An error occurred during preprocessing: {e}\", color=\"danger\")\n",
    "\n",
    "# --- Feature Selection and Visualization Callbacks ---\n",
    "@app.callback(\n",
    "    Output('feature-select-output', 'children'),\n",
    "    Input('run-feature-select-btn', 'n_clicks'),\n",
    "    [State('power-plant-selector', 'value'), State('target-selector', 'value')]\n",
    ")\n",
    "def run_feature_selection(n_clicks, power_plant, target_col):\n",
    "    if not n_clicks or not power_plant or not target_col:\n",
    "        return html.P(\"Please select a power plant and target variable, then click Analyze.\", className=\"text-muted\")\n",
    "    source = 'uploaded' if power_plant.startswith('uploaded_') else 'default'\n",
    "    actual_plant = power_plant.replace('uploaded_', '')\n",
    "    df, date_col, _ = load_plant_data(actual_plant, source)\n",
    "    if df.empty:\n",
    "        return dbc.Alert(\"No data available to analyze.\", color=\"warning\")\n",
    "    feature_cols, _, _, task_type = get_available_columns(df, date_col, actual_plant, source)\n",
    "    if not feature_cols:\n",
    "        return dbc.Alert(\"No feature columns found for analysis.\", color=\"warning\")\n",
    "    df_model = df[feature_cols + [target_col]].dropna()\n",
    "    X, y = df_model[feature_cols], df_model[target_col]\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    redundant_features = [col for col in upper_tri.columns if any(upper_tri[col] > 0.9)]\n",
    "    model = XGBRegressor(objective='reg:squarederror', n_estimators=30, max_depth=5, n_jobs=-1) if task_type == 'regression' else XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=30, max_depth=5, n_jobs=-1)\n",
    "    if task_type == 'classification':\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "    model.fit(X, y)\n",
    "    importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    return html.Div([\n",
    "        dbc.Row([\n",
    "            dbc.Col([html.H6(\"Feature Importance (from XGBoost)\"), dbc.Table.from_dataframe(importances.reset_index().rename(columns={'index': 'Feature', 0: 'Importance'}).head(10), striped=True)], width=6),\n",
    "            dbc.Col([html.H6(\"Potentially Redundant Features (Correlation > 0.9)\"), dbc.ListGroup([dbc.ListGroupItem(f) for f in redundant_features] if redundant_features else [dbc.ListGroupItem(\"None found.\")])], width=6)\n",
    "        ])\n",
    "    ])\n",
    "\n",
    "@app.callback(\n",
    "    Output('plot-controls', 'children'),\n",
    "    Input('plot-type-selector', 'value'),\n",
    "    State('power-plant-selector', 'value')\n",
    ")\n",
    "def update_plot_controls(plot_type, power_plant):\n",
    "    if not power_plant:\n",
    "        return None\n",
    "    source = 'uploaded' if power_plant.startswith('uploaded_') else 'default'\n",
    "    actual_plant = power_plant.replace('uploaded_', '')\n",
    "    df, _, _ = load_plant_data(actual_plant, source)\n",
    "    if df.empty:\n",
    "        return None\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    if plot_type == 'Histogram':\n",
    "        return dcc.Dropdown(id='hist-col', options=[{'label': col, 'value': col} for col in numeric_cols], value=numeric_cols[0] if numeric_cols else None, placeholder=\"Select column\")\n",
    "    if plot_type == 'Scatter Plot':\n",
    "        return [\n",
    "            dcc.Dropdown(id='scatter-x', options=[{'label': col, 'value': col} for col in numeric_cols], value=numeric_cols[0] if len(numeric_cols) > 0 else None, placeholder=\"X-axis\"),\n",
    "            dcc.Dropdown(id='scatter-y', options=[{'label': col, 'value': col} for col in numeric_cols], value=numeric_cols[1] if len(numeric_cols) > 1 else None, placeholder=\"Y-axis\")\n",
    "        ]\n",
    "    return None\n",
    "\n",
    "@app.callback(\n",
    "    Output('eda-plot', 'figure'),\n",
    "    [Input('plot-type-selector', 'value'), Input('hist-col', 'value'), Input('scatter-x', 'value'), Input('scatter-y', 'value')],\n",
    "    [State('power-plant-selector', 'value')],\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def update_eda_plot(plot_type, hist_col, scatter_x, scatter_y, power_plant):\n",
    "    try:\n",
    "        if not power_plant:\n",
    "            return go.Figure()\n",
    "        source = 'uploaded' if power_plant.startswith('uploaded_') else 'default'\n",
    "        actual_plant = power_plant.replace('uploaded_', '')\n",
    "        df, _, _ = load_plant_data(actual_plant, source)\n",
    "        if df.empty:\n",
    "            return go.Figure()\n",
    "        if plot_type == 'Histogram' and hist_col:\n",
    "            return px.histogram(df, x=hist_col, title=f'Distribution of {hist_col}')\n",
    "        if plot_type == 'Scatter Plot' and scatter_x and scatter_y:\n",
    "            return px.scatter(df, x=scatter_x, y=scatter_y, title=f'{scatter_x} vs. {scatter_y}')\n",
    "        if plot_type == 'Correlation Heatmap':\n",
    "            numeric_df = df.select_dtypes(include=np.number)\n",
    "            if numeric_df.empty:\n",
    "                return go.Figure()\n",
    "            return px.imshow(numeric_df.corr(), text_auto=True, title='Feature Correlation Heatmap')\n",
    "        return go.Figure()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in update_eda_plot: {e}\")\n",
    "        return go.Figure()\n",
    "\n",
    "# --- UI and Download Callbacks ---\n",
    "@app.callback(\n",
    "    Output('target-selector', 'options'),\n",
    "    Output('target-selector', 'value'),\n",
    "    Input('power-plant-selector', 'value')\n",
    ")\n",
    "def update_target_dropdown(pp):\n",
    "    if not pp:\n",
    "        return [], None\n",
    "    s = 'uploaded' if pp.startswith('uploaded_') else 'default'\n",
    "    ap = pp.replace('uploaded_', '')\n",
    "    df, dc, _ = load_plant_data(ap, s)\n",
    "    if df.empty:\n",
    "        return [], None\n",
    "    _, to, tc, _ = get_available_columns(df, dc, ap, s)\n",
    "    return [{'label': c, 'value': c} for c in to], tc\n",
    "\n",
    "@app.callback(\n",
    "    Output('daily-options', 'style'),\n",
    "    Output('month-year-options', 'style'),\n",
    "    Input('horizon-period-selector', 'value')\n",
    ")\n",
    "def toggle_horizon_options(p):\n",
    "    ds = {'display': 'block' if p == 'daily' else 'none', 'marginTop': '5px'}\n",
    "    ms = {'display': 'block' if p in ['daily', 'weekly', 'monthly'] else 'none', 'marginTop': '5px'}\n",
    "    return ds, ms\n",
    "\n",
    "@app.callback(\n",
    "    Output('view-data-container', 'style'),\n",
    "    Output('view-data-display', 'children'),\n",
    "    Input('view-btn', 'n_clicks'),\n",
    "    Input('close-view-btn', 'n_clicks'),\n",
    "    State('view-data-container', 'style'),\n",
    "    State('intermediate-data-store', 'data'),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def toggle_and_display_forecast_data(view_clicks, close_clicks, current_style, stored_data):\n",
    "    ctx = dash.callback_context\n",
    "    triggered_id = ctx.triggered[0]['prop_id'].split('.')[0]\n",
    "    if triggered_id == 'view-btn' and view_clicks and current_style.get('display') == 'none':\n",
    "        if not stored_data or 'forecast' not in stored_data:\n",
    "            return {'display': 'block', 'padding': '20px'}, html.P(\"Forecast data not available.\")\n",
    "        try:\n",
    "            df = pd.read_json(stored_data['forecast'], orient='split')\n",
    "            df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "            children = dash_table.DataTable(data=df.to_dict('records'), columns=[{'name': i, 'id': i} for i in df.columns], page_size=10, style_table={'overflowX': 'auto'})\n",
    "            return {'display': 'block', 'padding': '20px'}, children\n",
    "        except Exception as e:\n",
    "            return {'display': 'block', 'padding': '20px'}, html.P(f\"Could not display data. Error: {e}\")\n",
    "    if triggered_id == 'close-view-btn' and close_clicks:\n",
    "        return {'display': 'none'}, None\n",
    "    return current_style, dash.no_update\n",
    "\n",
    "@app.callback(\n",
    "    Output('data-display', 'children'),\n",
    "    Input('power-plant-selector', 'value'),\n",
    "    Input('data-display-selector', 'value')\n",
    ")\n",
    "def update_data_display(pp, o):\n",
    "    if not pp:\n",
    "        return \"Select a plant.\"\n",
    "    s = 'uploaded' if pp.startswith('uploaded_') else 'default'\n",
    "    ap = pp.replace('uploaded_', '')\n",
    "    df, _, e = load_plant_data(ap, s)\n",
    "    if df.empty:\n",
    "        return f\"Error: {e}\"\n",
    "    t = f\"Inspector for {ap}\"\n",
    "    if o == 'head':\n",
    "        return html.Div([html.H5(t), dash_table.DataTable(data=df.head().to_dict('records'), columns=[{'name': i, 'id': i} for i in df.columns])])\n",
    "    if o == 'tail':\n",
    "        return html.Div([html.H5(t), dash_table.DataTable(data=df.tail().to_dict('records'), columns=[{'name': i, 'id': i} for i in df.columns])])\n",
    "    if o == 'info':\n",
    "        b = io.StringIO()\n",
    "        df.info(buf=b)\n",
    "        return html.Div([html.H5(t), html.Pre(b.getvalue())])\n",
    "    if o == 'describe':\n",
    "        return html.Div([html.H5(t), dash_table.DataTable(data=df.describe().reset_index().to_dict('records'), columns=[{'name': i, 'id': i} for i in df.describe().reset_index().columns])])\n",
    "    if o == 'full':\n",
    "        df_c = df.copy()\n",
    "        for col in df_c.select_dtypes(include=['datetime64[ns]']).columns:\n",
    "            df_c[col] = df_c[col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        return html.Div([html.H5(t), dash_table.DataTable(data=df_c.to_dict('records'), columns=[{'name': i, 'id': i} for i in df_c.columns], page_size=15)])\n",
    "    return None\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"download-forecast\", \"data\"),\n",
    "    Input(\"download-btn\", \"n_clicks\"),\n",
    "    State('intermediate-data-store', 'data'),\n",
    "    State('power-plant-selector', 'value'),\n",
    "    State('target-selector', 'value'),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def download_csv(n, d, pp, t):\n",
    "    if not d or 'forecast' not in d:\n",
    "        return None\n",
    "    df = pd.read_json(d['forecast'], orient='split')\n",
    "    return dcc.send_data_frame(df.to_csv, f\"{pp.replace('uploaded_','')}_{t}_forecast.csv\", index=False)\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"download-pdf\", \"data\"),\n",
    "    Input(\"download-pdf-btn\", \"n_clicks\"),\n",
    "    State('intermediate-data-store', 'data'),\n",
    "    State('power-plant-selector', 'value'),\n",
    "    State('target-selector', 'value'),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def download_analysis_pdf(n_clicks, stored_data, power_plant, target):\n",
    "    if not n_clicks or not stored_data or 'forecast' not in stored_data:\n",
    "        return None\n",
    "    try:\n",
    "        forecast_df = pd.read_json(stored_data['forecast'], orient='split')\n",
    "        forecast_df['Date'] = pd.to_datetime(forecast_df['Date']).dt.strftime('%Y-%m-%d')\n",
    "        metrics_data = json.loads(stored_data['metrics'])\n",
    "        historical_df = pd.read_json(stored_data['historical'], orient='split')\n",
    "        historical_df.columns = ['Date', 'Actual']\n",
    "        historical_df['Date'] = pd.to_datetime(historical_df['Date']).dt.strftime('%Y-%m-%d')\n",
    "        best_model_name = stored_data['best_model']\n",
    "        task_type = stored_data['task_type']\n",
    "\n",
    "        pdf_buffer = io.BytesIO()\n",
    "        doc = SimpleDocTemplate(pdf_buffer, pagesize=letter)\n",
    "        styles = getSampleStyleSheet()\n",
    "        elements = []\n",
    "\n",
    "        elements.append(Paragraph(f\"Analysis & Decision-Making Report\", styles['h1']))\n",
    "        elements.append(Paragraph(f\"<b>Power Plant:</b> {power_plant.replace('uploaded_','')} | <b>Target:</b> {target}\", styles['h2']))\n",
    "        elements.append(Spacer(1, 12))\n",
    "        elements.append(Paragraph(f\"<b>Date Generated:</b> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", styles['Normal']))\n",
    "        elements.append(Paragraph(f\"<b>Best Performing Model:</b> {best_model_name}\", styles['Normal']))\n",
    "        elements.append(Spacer(1, 24))\n",
    "\n",
    "        elements.append(Paragraph(\"Model Performance Metrics\", styles['h3']))\n",
    "        if metrics_data:\n",
    "            headers = list(metrics_data[0].keys())\n",
    "            table_data = [headers] + [[str(row.get(col, '')) for col in headers] for row in metrics_data]\n",
    "            style = TableStyle([\n",
    "                ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue), ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "                ('ALIGN', (0, 0), (-1, -1), 'CENTER'), ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "                ('BOTTOMPADDING', (0, 0), (-1, 0), 12), ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n",
    "                ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "            ])\n",
    "            for i, row in enumerate(metrics_data):\n",
    "                if row.get('Best') == '‚òÖ':\n",
    "                    style.add('BACKGROUND', (0, i + 1), (-1, i + 1), colors.lightgreen)\n",
    "            metrics_table = Table(table_data)\n",
    "            metrics_table.setStyle(style)\n",
    "            elements.append(metrics_table)\n",
    "        elements.append(Spacer(1, 24))\n",
    "\n",
    "        elements.append(Paragraph(\"Forecast Data Summary\", styles['h3']))\n",
    "        forecast_table_data = [forecast_df.columns.tolist()] + forecast_df.head().values.tolist()\n",
    "        forecast_table = Table(forecast_table_data)\n",
    "        forecast_table.setStyle(TableStyle([\n",
    "            ('BACKGROUND', (0, 0), (-1, 0), colors.grey), ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "            ('ALIGN', (0, 0), (-1, -1), 'CENTER'), ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "        ]))\n",
    "        elements.append(forecast_table)\n",
    "        elements.append(Paragraph(f\"(Showing first 5 of {len(forecast_df)} forecasted points)\", styles['Italic']))\n",
    "        elements.append(Spacer(1, 24))\n",
    "\n",
    "        elements.append(Paragraph(\"Automated Recommendations\", styles['h3']))\n",
    "        if task_type == 'regression':\n",
    "            forecast_values = forecast_df[f'Forecast_{target}'].values\n",
    "            slope, _ = np.polyfit(np.arange(len(forecast_values)), forecast_values, 1)\n",
    "            if slope > 0.05:\n",
    "                elements.append(Paragraph(\"‚Ä¢ <b>Uptrend Detected:</b> Forecast shows an increasing trend. Plan for higher levels/loads.\", styles['Normal']))\n",
    "            elif slope < -0.05:\n",
    "                elements.append(Paragraph(\"‚Ä¢ <b>Downtrend Detected:</b> Forecast indicates a decreasing trend. Prepare for reduced levels/loads.\", styles['Normal']))\n",
    "            else:\n",
    "                elements.append(Paragraph(\"‚Ä¢ <b>Stable Trend:</b> Forecast shows stable values. Maintain current operational plans.\", styles['Normal']))\n",
    "            forecast_variance = np.var(forecast_values)\n",
    "            if forecast_variance > np.var(historical_df['Actual']):\n",
    "                elements.append(Paragraph(\"‚Ä¢ <b>High Variance Alert:</b> Forecasted values show higher variability than historical data. Consider risk mitigation strategies.\", styles['Normal']))     \n",
    "            if 'anomaly_details' in stored_data and stored_data['anomaly_details']:\n",
    "                elements.append(Paragraph(\"‚Ä¢ <b>Anomaly Alerts:</b> High-deviation forecasts detected. Review the following for operational adjustments:\", styles['Normal']))\n",
    "                anomaly_data = stored_data['anomaly_details']\n",
    "                anomaly_table_data = [['GC Date Actual', 'Actual Data', 'Forecasted Data', 'Error', 'GC Date Forecast']] + [\n",
    "                    [row['gc_date_actual'], row['actual_data'], row['forecasted_data'], row['error'], row['gc_date_forecast']]\n",
    "                    for row in anomaly_data\n",
    "                ]\n",
    "                anomaly_table = Table(anomaly_table_data)\n",
    "                anomaly_table.setStyle(TableStyle([\n",
    "                    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
    "                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "                    ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "                ]))\n",
    "                elements.append(anomaly_table)\n",
    "            else:\n",
    "                elements.append(Paragraph(\"‚Ä¢ <b>No Anomalies:</b> No significant deviations detected in the forecast.\", styles['Normal']))\n",
    "\n",
    "        elements.append(Spacer(1, 24))\n",
    "        elements.append(Paragraph(\"Historical Data Summary\", styles['h3']))\n",
    "        historical_table_data = [historical_df.columns.tolist()] + historical_df.head().values.tolist()\n",
    "        historical_table = Table(historical_table_data)\n",
    "        historical_table.setStyle(TableStyle([\n",
    "            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
    "            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "        ]))\n",
    "        elements.append(historical_table)\n",
    "        elements.append(Paragraph(f\"(Showing first 5 of {len(historical_df)} historical points)\", styles['Italic']))\n",
    "\n",
    "        doc.build(elements)\n",
    "        pdf_buffer.seek(0)\n",
    "        return dcc.send_bytes(pdf_buffer.getvalue(), f\"{power_plant.replace('uploaded_','')}_{target}_analysis_report.pdf\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating PDF: {e}\")\n",
    "        return None\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(mode='inline', port=5021, debug=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Dash server: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccec4e6-af47-42ca-ac3a-7ee057700a04",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The `html.Div` component (version 3.1.1) received an unexpected keyword argument: `fluid`\nAllowed arguments: accessKey, aria-*, children, className, contentEditable, data-*, dir, disable_n_clicks, draggable, hidden, id, key, lang, n_clicks, n_clicks_timestamp, role, spellCheck, style, tabIndex, title",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 248\u001b[39m\n\u001b[32m    243\u001b[39m app.title = \u001b[33m\"\u001b[39m\u001b[33müöÄ Ethiopian Power Plant ML Dashboard - FULLY FUNCTIONAL\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# =================================================================================\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# === MAIN DASHBOARD LAYOUT ===\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# =================================================================================\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m app.layout = \u001b[43mhtml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDiv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Header\u001b[39;49;00m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mContainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mH1\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43müöÄ Ethiopian Power Plant ML Dashboard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m                \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-center mb-4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcolor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m#2c3e50\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfontWeight\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbold\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mP\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAdvanced Machine Learning Analysis with Auto-Target Detection & Model Comparison\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m               \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-center text-muted\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfontSize\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m1.2rem\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmb-5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Main Controls Row\u001b[39;49;00m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Left Column - Plant & Target Selection\u001b[39;49;00m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCol\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCardHeader\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43müè≠ Power Plant Selection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbg-primary text-white\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCardBody\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mdcc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDropdown\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mplant-selector\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m                        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mplant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mplant\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mplant\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPOWER_PLANT_DATA_FULL_ANALYSIS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGibe 1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mclearable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmb-3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    271\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mH6\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43müéØ Auto-Detected Target Variables\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-success\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtarget-variables-display\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmb-4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Right Column - Analysis Controls\u001b[39;49;00m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCol\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCardHeader\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m‚öôÔ∏è Analysis Configuration\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbg-success text-white\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCardBody\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mdcc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDropdown\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimputation-selector\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m                        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m                            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMedian (Recommended)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmedian\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m                            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m                            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAuto (Skew-Aware)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmedian\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mclearable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmb-3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    294\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mdcc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDropdown\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msplit-selector\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m                        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m                            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m70\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m Train / 30\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m Test\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m                            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m80\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m Train / 20\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m Test\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m                            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m90\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m Train / 10\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m Test\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mclearable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    304\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mButton\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43müöÄ RUN ANALYSIS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrun-analysis-btn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwarning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmt-3 w-100\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmb-4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Results Summary\u001b[39;49;00m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCol\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCardHeader\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mü•á BEST MODEL RESULTS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbg-info text-white\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCardBody\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest-model-results\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-center\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmb-4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmb-5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Results Tabs\u001b[39;49;00m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCardHeader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfas fa-chart-line me-2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSpan\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43müìä ANALYSIS RESULTS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbg-dark text-white\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCardBody\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43müìà Model Performance\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtab_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43müìã All Plants Summary\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtab_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummary\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43müîç Data Preview\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtab_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresults-tabs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactive_tab\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtab-content\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmb-4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Global Best Results\u001b[39;49;00m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCardHeader\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43müåü GLOBAL BEST RESULT ACROSS ALL PLANTS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbg-danger text-white\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCardBody\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mglobal-best-result\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfluid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# =================================================================================\u001b[39;00m\n\u001b[32m    347\u001b[39m \u001b[38;5;66;03m# === MAIN CALLBACKS ===\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# =================================================================================\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;129m@app\u001b[39m.callback(\n\u001b[32m    351\u001b[39m     Output(\u001b[33m'\u001b[39m\u001b[33mtarget-variables-display\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mchildren\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    352\u001b[39m     Input(\u001b[33m'\u001b[39m\u001b[33mplant-selector\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    353\u001b[39m )\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_target_display\u001b[39m(plant_name):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\dash\\development\\base_component.py:475\u001b[39m, in \u001b[36m_explicitize_args.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33m_explicit_args\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    474\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33m_explicit_args\u001b[39m\u001b[33m\"\u001b[39m].remove(\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\dash\\html\\Div.py:176\u001b[39m, in \u001b[36mDiv.__init__\u001b[39m\u001b[34m(self, children, id, n_clicks, n_clicks_timestamp, disable_n_clicks, key, accessKey, className, contentEditable, dir, draggable, hidden, lang, role, spellCheck, style, tabIndex, title, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m _locals.update(kwargs)  \u001b[38;5;66;03m# For wildcard attrs and excess named props\u001b[39;00m\n\u001b[32m    174\u001b[39m args = {k: _locals[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _explicit_args \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[33m\"\u001b[39m\u001b[33mchildren\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDiv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\dash\\development\\base_component.py:174\u001b[39m, in \u001b[36mComponent.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k_in_propnames \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k_in_wildcards:\n\u001b[32m    171\u001b[39m     allowed_args = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    172\u001b[39m         \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m._prop_names)\n\u001b[32m    173\u001b[39m     )  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    175\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_string_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m received an unexpected keyword argument: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAllowed arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._base_nodes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Component):\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    181\u001b[39m         error_string_prefix\n\u001b[32m    182\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33m detected a Component for a prop other than `children`\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    185\u001b[39m         + \u001b[33m'\u001b[39m\u001b[33mFor example, it must be html.Div([\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mc\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m]) not html.Div(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mc\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    186\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: The `html.Div` component (version 3.1.1) received an unexpected keyword argument: `fluid`\nAllowed arguments: accessKey, aria-*, children, className, contentEditable, data-*, dir, disable_n_clicks, draggable, hidden, id, key, lang, n_clicks, n_clicks_timestamp, role, spellCheck, style, tabIndex, title"
     ]
    }
   ],
   "source": [
    "# Install required packages (run this in a separate cell if not already installed)\n",
    "# !pip install xgboost jupyter_dash dash dash-bootstrap-components pandas numpy plotly scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from io import StringIO\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output, State, dash_table, callback_context\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "import base64\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =================================================================================\n",
    "# === HARD-CODED POWER PLANT DATA CONFIGURATION (FULLY FUNCTIONAL) ===\n",
    "# =================================================================================\n",
    "POWER_PLANT_DATA_FULL_ANALYSIS = {\n",
    "    'Gibe 1': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Gibe1.csv\",\n",
    "        'target_vars': ['Water_Level', 'Total_pr', 'Max_ALoad', 'Min_ALoad'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'Gibe 2': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Gibe2.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max_load', 'min_load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'Gibe3': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Gibe3.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max_load', 'min_load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'Amerti': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Amerti Neshi.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max_load', 'min_load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'GERD': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\GERD.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max_load', 'min_load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'Finchaa': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\fincha.csv\",\n",
    "        'target_vars': ['water level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'Koka': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Koka Plant.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max_load', 'min_load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'Tana Beles': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Tana_Beles.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'Tekeze': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Tekeze.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'Awash 2': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Awash2.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'Awash 3': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Awash3.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'Melka Wakena': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\wakena.csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    },\n",
    "    'Genale': {\n",
    "        'path': r\"C:\\Users\\hp\\Documents\\code for internship 3rd year\\CLEANED EEP DATASET 2017\\data last\\Genale .csv\",\n",
    "        'target_vars': ['water_level', 'total-pr', 'max load', 'min load'],\n",
    "        'date_col': 'Date_GC'\n",
    "    }\n",
    "}\n",
    "\n",
    "ALLOWED_TARGET_KEYWORDS = ['level', 'pr', 'energy', 'discharge', 'auxiliary', 'load', 'u1', 'u2', 'u3', 'u4', 'u5', 'u6']\n",
    "\n",
    "# Global cache\n",
    "data_cache = {}\n",
    "GLOBAL_BEST_RESULTS = {'R2_Test': -np.inf, 'Power Plant': 'N/A', 'Model': 'N/A', 'Split': 'N/A', 'Imputation Method': 'N/A'}\n",
    "ALL_PLANTS_RESULTS = {}\n",
    "\n",
    "# =================================================================================\n",
    "# === CORE DATA LOADING & PREPROCESSING FUNCTIONS ===\n",
    "# =================================================================================\n",
    "def load_plant_data(plant_name):\n",
    "    \"\"\"Load and preprocess data for a specific power plant\"\"\"\n",
    "    if plant_name not in POWER_PLANT_DATA_FULL_ANALYSIS:\n",
    "        return pd.DataFrame(), None, \"Plant not found\"\n",
    "    \n",
    "    config = POWER_PLANT_DATA_FULL_ANALYSIS[plant_name]\n",
    "    file_path = config['path']\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        return pd.DataFrame(), None, f\"File not found: {file_path}\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        date_col = config['date_col']\n",
    "        \n",
    "        if date_col in df.columns:\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "            df = df.dropna(subset=[date_col]).sort_values(date_col).reset_index(drop=True)\n",
    "        \n",
    "        # Auto-detect target columns from hard-coded list first, then keywords\n",
    "        target_candidates = []\n",
    "        for target in config['target_vars']:\n",
    "            if target in df.columns:\n",
    "                target_candidates.append(target)\n",
    "        \n",
    "        if not target_candidates:\n",
    "            target_candidates = [col for col in df.columns \n",
    "                               if any(keyword in col.lower() for keyword in ALLOWED_TARGET_KEYWORDS) \n",
    "                               and pd.api.types.is_numeric_dtype(df[col])]\n",
    "        \n",
    "        return df, date_col, target_candidates[0] if target_candidates else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return pd.DataFrame(), None, f\"Error loading data: {str(e)}\"\n",
    "\n",
    "def preprocess_data(df, target_col, imputation_method='median'):\n",
    "    \"\"\"Preprocess data with selected imputation method\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Drop columns with >90% missing values\n",
    "    missing_pct = df_processed.isnull().sum() / len(df_processed)\n",
    "    cols_to_drop = missing_pct[missing_pct > 0.9].index\n",
    "    df_processed.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Impute missing values\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col == target_col:\n",
    "            continue\n",
    "        if imputation_method == 'mean':\n",
    "            df_processed[col].fillna(df_processed[col].mean(), inplace=True)\n",
    "        elif imputation_method == 'median':\n",
    "            df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "        elif imputation_method == 'auto':\n",
    "            if abs(df_processed[col].skew()) > 1.0:\n",
    "                df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "            else:\n",
    "                df_processed[col].fillna(df_processed[col].mean(), inplace=True)\n",
    "        else:\n",
    "            df_processed[col].fillna(0, inplace=True)\n",
    "    \n",
    "    # Impute target column\n",
    "    if target_col in df_processed.columns:\n",
    "        non_null_target = df_processed[target_col].dropna()\n",
    "        if not non_null_target.empty:\n",
    "            if imputation_method == 'mean':\n",
    "                df_processed[target_col].fillna(non_null_target.mean(), inplace=True)\n",
    "            elif imputation_method == 'median':\n",
    "                df_processed[target_col].fillna(non_null_target.median(), inplace=True)\n",
    "    \n",
    "    return df_processed.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "\n",
    "# =================================================================================\n",
    "# === MODEL TRAINING FUNCTION (FROM ORIGINAL JUPYTER CODE) ===\n",
    "# =================================================================================\n",
    "def run_regression_analysis_detailed(X, y, split_ratio):\n",
    "    models = {\n",
    "        'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf', C=1.0),\n",
    "        'KNN': KNeighborsRegressor(n_neighbors=5),\n",
    "        'XGBoost': XGBRegressor(n_estimators=30, learning_rate=0.1, max_depth=5, n_jobs=-1, random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=30, max_depth=10, n_jobs=-1, random_state=42),\n",
    "        'Linear Regression': LinearRegression()\n",
    "    }\n",
    "    results = []\n",
    "    \n",
    "    if len(y) < 20:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_normalized = pd.DataFrame(scaler_X.fit_transform(X), columns=X.columns, index=X.index)\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_normalized = pd.Series(scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten(), index=y.index)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_normalized, y_normalized, test_size=1 - split_ratio, shuffle=False\n",
    "    )\n",
    "    \n",
    "    if len(y_train) == 0 or len(y_test) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "            \n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'R2_Train': max(0, r2_score(y_train, y_train_pred)),\n",
    "                'R2_Test': max(0, r2_score(y_test, y_test_pred)),\n",
    "                'MAE_Train': mean_absolute_error(y_train, y_train_pred),\n",
    "                'MAE_Test': mean_absolute_error(y_test, y_test_pred),\n",
    "                'RMSE_Train': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "                'RMSE_Test': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "                'Corr_Train': pd.Series(y_train).corr(pd.Series(y_train_pred)) if np.var(y_train) > 0 and np.var(y_train_pred) > 0 else 0,\n",
    "                'Corr_Test': pd.Series(y_test).corr(pd.Series(y_test_pred)) if np.var(y_test) > 0 and np.var(y_test_pred) > 0 else 0\n",
    "            })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    if not results_df.empty:\n",
    "        results_df['Rank (R2_Test)'] = results_df['R2_Test'].rank(method='min', ascending=False).astype(int)\n",
    "        results_df = results_df.sort_values('R2_Test', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# =================================================================================\n",
    "# === DASH APP INITIALIZATION ===\n",
    "# =================================================================================\n",
    "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "app.title = \"üöÄ Ethiopian Power Plant ML Dashboard - FULLY FUNCTIONAL\"\n",
    "\n",
    "# =================================================================================\n",
    "# === MAIN DASHBOARD LAYOUT ===\n",
    "# =================================================================================\n",
    "app.layout = html.Div([\n",
    "    # Header\n",
    "    dbc.Container([\n",
    "        html.H1(\"üöÄ Ethiopian Power Plant ML Dashboard\", \n",
    "                className=\"text-center mb-4\", \n",
    "                style={'color': '#2c3e50', 'fontWeight': 'bold'}),\n",
    "        html.P(\"Advanced Machine Learning Analysis with Auto-Target Detection & Model Comparison\",\n",
    "               className=\"text-center text-muted\", style={'fontSize': '1.2rem'})\n",
    "    ], className=\"mb-5\"),\n",
    "    \n",
    "    # Main Controls Row\n",
    "    dbc.Row([\n",
    "        # Left Column - Plant & Target Selection\n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardHeader(\"üè≠ Power Plant Selection\", className=\"bg-primary text-white\"),\n",
    "                dbc.CardBody([\n",
    "                    dcc.Dropdown(\n",
    "                        id='plant-selector',\n",
    "                        options=[{'label': plant, 'value': plant} for plant in POWER_PLANT_DATA_FULL_ANALYSIS.keys()],\n",
    "                        value='Gibe 1',\n",
    "                        clearable=False,\n",
    "                        className=\"mb-3\"\n",
    "                    ),\n",
    "                    html.Hr(),\n",
    "                    html.H6(\"üéØ Auto-Detected Target Variables\", className=\"text-success\"),\n",
    "                    html.Div(id='target-variables-display')\n",
    "                ])\n",
    "            ], className=\"mb-4\")\n",
    "        ], width=4),\n",
    "        \n",
    "        # Right Column - Analysis Controls\n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardHeader(\"‚öôÔ∏è Analysis Configuration\", className=\"bg-success text-white\"),\n",
    "                dbc.CardBody([\n",
    "                    dcc.Dropdown(\n",
    "                        id='imputation-selector',\n",
    "                        options=[\n",
    "                            {'label': 'Median (Recommended)', 'value': 'median'},\n",
    "                            {'label': 'Mean', 'value': 'mean'},\n",
    "                            {'label': 'Auto (Skew-Aware)', 'value': 'auto'}\n",
    "                        ],\n",
    "                        value='median',\n",
    "                        clearable=False,\n",
    "                        className=\"mb-3\"\n",
    "                    ),\n",
    "                    dcc.Dropdown(\n",
    "                        id='split-selector',\n",
    "                        options=[\n",
    "                            {'label': '70% Train / 30% Test', 'value': 0.7},\n",
    "                            {'label': '80% Train / 20% Test', 'value': 0.8},\n",
    "                            {'label': '90% Train / 10% Test', 'value': 0.9}\n",
    "                        ],\n",
    "                        value=0.8,\n",
    "                        clearable=False\n",
    "                    ),\n",
    "                    dbc.Button(\"üöÄ RUN ANALYSIS\", id='run-analysis-btn', color=\"warning\", className=\"mt-3 w-100\", size=\"lg\")\n",
    "                ])\n",
    "            ], className=\"mb-4\")\n",
    "        ], width=4),\n",
    "        \n",
    "        # Results Summary\n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardHeader(\"ü•á BEST MODEL RESULTS\", className=\"bg-info text-white\"),\n",
    "                dbc.CardBody([\n",
    "                    html.Div(id='best-model-results', className=\"text-center\")\n",
    "                ])\n",
    "            ], className=\"mb-4\")\n",
    "        ], width=4)\n",
    "    ], className=\"mb-5\"),\n",
    "    \n",
    "    # Results Tabs\n",
    "    dbc.Card([\n",
    "        dbc.CardHeader([\n",
    "            html.I(className=\"fas fa-chart-line me-2\"),\n",
    "            html.Span(\"üìä ANALYSIS RESULTS\")\n",
    "        ], className=\"bg-dark text-white\"),\n",
    "        dbc.CardBody([\n",
    "            dbc.Tabs([\n",
    "                dbc.Tab(label=\"üìà Model Performance\", tab_id=\"models\"),\n",
    "                dbc.Tab(label=\"üìã All Plants Summary\", tab_id=\"summary\"),\n",
    "                dbc.Tab(label=\"üîç Data Preview\", tab_id=\"data\")\n",
    "            ], id=\"results-tabs\", active_tab=\"models\"),\n",
    "            html.Div(id=\"tab-content\")\n",
    "        ])\n",
    "    ], className=\"mb-4\"),\n",
    "    \n",
    "    # Global Best Results\n",
    "    dbc.Card([\n",
    "        dbc.CardHeader(\"üåü GLOBAL BEST RESULT ACROSS ALL PLANTS\", className=\"bg-danger text-white\"),\n",
    "        dbc.CardBody([\n",
    "            html.Div(id='global-best-result')\n",
    "        ])\n",
    "    ])\n",
    "], fluid=True)\n",
    "\n",
    "# =================================================================================\n",
    "# === MAIN CALLBACKS ===\n",
    "# =================================================================================\n",
    "\n",
    "@app.callback(\n",
    "    Output('target-variables-display', 'children'),\n",
    "    Input('plant-selector', 'value')\n",
    ")\n",
    "def update_target_display(plant_name):\n",
    "    if not plant_name:\n",
    "        return html.P(\"Select a power plant to see available targets\", className=\"text-muted\")\n",
    "    \n",
    "    df, date_col, default_target = load_plant_data(plant_name)\n",
    "    config = POWER_PLANT_DATA_FULL_ANALYSIS.get(plant_name, {})\n",
    "    targets = config.get('target_vars', [])\n",
    "    \n",
    "    if not targets:\n",
    "        return html.P(\"No predefined targets found\", className=\"text-warning\")\n",
    "    \n",
    "    target_buttons = []\n",
    "    for i, target in enumerate(targets):\n",
    "        color = \"success\" if i == 0 else \"secondary\"\n",
    "        target_buttons.append(\n",
    "            dbc.Button(\n",
    "                f\"üéØ {target}\",\n",
    "                id={'type': 'target-btn', 'index': i},\n",
    "                color=color,\n",
    "                className=\"me-2 mb-2\",\n",
    "                size=\"sm\"\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return html.Div([\n",
    "        html.P(f\"Default Target: <strong>{default_target}</strong>\", className=\"text-primary fw-bold\"),\n",
    "        html.Div(target_buttons, className=\"mt-3\")\n",
    "    ])\n",
    "\n",
    "@app.callback(\n",
    "    Output('tab-content', 'children'),\n",
    "    Output('best-model-results', 'children'),\n",
    "    Output('global-best-result', 'children'),\n",
    "    Input('run-analysis-btn', 'n_clicks'),\n",
    "    State('plant-selector', 'value'),\n",
    "    State('imputation-selector', 'value'),\n",
    "    State('split-selector', 'value')\n",
    ")\n",
    "def run_full_analysis(n_clicks, plant_name, imputation_method, split_ratio):\n",
    "    if not n_clicks or not plant_name:\n",
    "        return (html.P(\"Click 'RUN ANALYSIS' to start\", className=\"text-muted\"), \n",
    "                html.P(\"No results yet\", className=\"text-muted\"), \n",
    "                html.P(\"No global best yet\", className=\"text-muted\"))\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df_raw, date_col, default_target = load_plant_data(plant_name)\n",
    "    if df_raw.empty or default_target is None:\n",
    "        return (dbc.Alert(\"‚ùå No valid data or target found\", color=\"danger\"),\n",
    "                html.P(\"\", className=\"text-muted\"),\n",
    "                html.P(\"\", className=\"text-muted\"))\n",
    "    \n",
    "    df_processed = preprocess_data(df_raw, default_target, imputation_method)\n",
    "    \n",
    "    if len(df_processed) < 20:\n",
    "        return (dbc.Alert(\"‚ùå Insufficient data for modeling\", color=\"warning\"),\n",
    "                html.P(\"\", className=\"text-muted\"),\n",
    "                html.P(\"\", className=\"text-muted\"))\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_cols = [col for col in df_processed.select_dtypes(include=np.number).columns \n",
    "                    if col != default_target]\n",
    "    X = df_processed[feature_cols]\n",
    "    y = df_processed[default_target]\n",
    "    \n",
    "    # Run model analysis\n",
    "    results_df = run_regression_analysis_detailed(X, y, split_ratio)\n",
    "    \n",
    "    if results_df.empty:\n",
    "        return (dbc.Alert(\"‚ùå No models could be trained\", color=\"danger\"),\n",
    "                html.P(\"\", className=\"text-muted\"),\n",
    "                html.P(\"\", className=\"text-muted\"))\n",
    "    \n",
    "    # Update global best\n",
    "    global GLOBAL_BEST_RESULTS, ALL_PLANTS_RESULTS\n",
    "    best_row = results_df.iloc[0]\n",
    "    if best_row['R2_Test'] > GLOBAL_BEST_RESULTS['R2_Test']:\n",
    "        GLOBAL_BEST_RESULTS = {\n",
    "            'R2_Test': best_row['R2_Test'],\n",
    "            'Power Plant': plant_name,\n",
    "            'Model': best_row['Model'],\n",
    "            'Split': f\"{int(split_ratio*100)}% Train\",\n",
    "            'Imputation Method': imputation_method\n",
    "        }\n",
    "    \n",
    "    ALL_PLANTS_RESULTS[plant_name] = {\n",
    "        'Plant': plant_name,\n",
    "        'Best Model': best_row['Model'],\n",
    "        'R2 Test': best_row['R2_Test'],\n",
    "        'Split': f\"{int(split_ratio*100)}%\",\n",
    "        'Imputation': imputation_method\n",
    "    }\n",
    "    \n",
    "    # Model results table\n",
    "    model_table = dash_table.DataTable(\n",
    "        data=results_df.round(4).to_dict('records'),\n",
    "        columns=[{'name': i, 'id': i} for i in results_df.columns],\n",
    "        page_size=10,\n",
    "        style_cell={'textAlign': 'left', 'fontSize': '12px'},\n",
    "        style_data_conditional=[\n",
    "            {'if': {'row_index': 0}, 'backgroundColor': '#d4edda', 'color': 'darkgreen', 'fontWeight': 'bold'},\n",
    "            {'if': {'column_id': 'R2_Test'}, 'fontWeight': 'bold'}\n",
    "        ],\n",
    "        style_header={'backgroundColor': '#007bff', 'color': 'white', 'fontWeight': 'bold'}\n",
    "    )\n",
    "    \n",
    "    # Current plant best result\n",
    "    best_result_card = dbc.Card([\n",
    "        dbc.CardBody([\n",
    "            html.H5(f\"ü•á {best_row['Model']}\", className=\"text-success\"),\n",
    "            html.H6(f\"R¬≤ Test: {best_row['R2_Test']:.4f}\", className=\"text-primary\"),\n",
    "            html.P(f\"Train/Test Split: {int(split_ratio*100)}% / {100-int(split_ratio*100)}%\"),\n",
    "            html.P(f\"Imputation: {imputation_method}\")\n",
    "        ])\n",
    "    ], className=\"text-center\")\n",
    "    \n",
    "    # Global best result\n",
    "    global_best_card = dbc.Card([\n",
    "        dbc.CardBody([\n",
    "            html.H5(f\"üåü {GLOBAL_BEST_RESULTS['Model']}\", className=\"text-danger\"),\n",
    "            html.H6(f\"R¬≤ Test: {GLOBAL_BEST_RESULTS['R2_Test']:.4f}\", className=\"text-warning\"),\n",
    "            html.P(f\"Plant: {GLOBAL_BEST_RESULTS['Power Plant']}\"),\n",
    "            html.P(f\"Split: {GLOBAL_BEST_RESULTS['Split']}\")\n",
    "        ])\n",
    "    ], className=\"text-center\")\n",
    "    \n",
    "    # All plants summary\n",
    "    if ALL_PLANTS_RESULTS:\n",
    "        summary_df = pd.DataFrame(list(ALL_PLANTS_RESULTS.values()))\n",
    "        summary_table = dash_table.DataTable(\n",
    "            data=summary_df.round(4).to_dict('records'),\n",
    "            columns=[{'name': i, 'id': i} for i in summary_df.columns],\n",
    "            page_size=10,\n",
    "            style_cell_conditional=[\n",
    "                {'if': {'column_id': 'R2 Test'}, 'fontWeight': 'bold'}\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        summary_table = html.P(\"No other plants analyzed yet\")\n",
    "    \n",
    "    tab_content = html.Div([\n",
    "        dbc.Tabs([\n",
    "            dbc.Tab(label=\"üìà Model Performance\", tab_id=\"models\", children=[model_table]),\n",
    "            dbc.Tab(label=\"üìã All Plants Summary\", tab_id=\"summary\", children=[summary_table]),\n",
    "            dbc.Tab(label=\"üîç Data Preview\", tab_id=\"data\", children=[\n",
    "                html.H6(f\"Plant: {plant_name} | Target: {default_target}\"),\n",
    "                dash_table.DataTable(\n",
    "                    data=df_processed[[default_target] + feature_cols[:5]].head(10).round(2).to_dict('records'),\n",
    "                    columns=[{'name': i, 'id': i} for i in df_processed[[default_target] + feature_cols[:5]].head(10).columns],\n",
    "                    page_size=10\n",
    "                )\n",
    "            ])\n",
    "        ])\n",
    "    ])\n",
    "    \n",
    "    return tab_content, best_result_card, global_best_card\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(mode='inline', debug=True, port=6051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b64748-e44a-4339-9e43-8fe2aa34e6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
